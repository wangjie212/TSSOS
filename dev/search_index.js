var documenterSearchIndex = {"docs":
[{"location":"pmo/#Polynomial-Matrix-Optimization","page":"Polynomial Matrix Optimization","title":"Polynomial Matrix Optimization","text":"","category":"section"},{"location":"pmo/","page":"Polynomial Matrix Optimization","title":"Polynomial Matrix Optimization","text":"The polynomial matrix optimization problem aims to minimize the smallest eigenvalue of a polynomial matrix subject to a tuple of polynomial matrix inequalties (PMIs), which could be formulized as","category":"page"},{"location":"pmo/","page":"Polynomial Matrix Optimization","title":"Polynomial Matrix Optimization","text":"mathrminf_mathbfxinmathbfK lambda_mathrmmin(F(mathbfx))","category":"page"},{"location":"pmo/","page":"Polynomial Matrix Optimization","title":"Polynomial Matrix Optimization","text":"where FinmathbbSmathbfx^p is a ptimes p symmetric polynomial matrix and mathbfK is the basic semialgebraic set","category":"page"},{"location":"pmo/","page":"Polynomial Matrix Optimization","title":"Polynomial Matrix Optimization","text":"mathbfKcoloneqqlbrace mathbfxinmathbbR^n mid G_j(mathbfx)succeq0 j=1ldotsmrbrace","category":"page"},{"location":"pmo/","page":"Polynomial Matrix Optimization","title":"Polynomial Matrix Optimization","text":"for some symmetric polynomial matrices G_jinmathbbSmathbfx^q_j j=1ldotsm. Note that when p=1, lambda_min(F(mathbfx))=F(mathbfx). More generally, one may consider","category":"page"},{"location":"pmo/","page":"Polynomial Matrix Optimization","title":"Polynomial Matrix Optimization","text":"mathrminf_mathbfyinmathbbR^t mathbfc^intercalmathbfy","category":"page"},{"location":"pmo/","page":"Polynomial Matrix Optimization","title":"Polynomial Matrix Optimization","text":"mathrmst F_0(mathbfx)+y_1F_1(mathbfx)+cdots+y_tF_t(mathbfx)succeq0 textrm on  K","category":"page"},{"location":"pmo/","page":"Polynomial Matrix Optimization","title":"Polynomial Matrix Optimization","text":"where F_iinmathbbSmathbfx^p i=01ldotst are a tuple of symmetric polynomial matrices.","category":"page"},{"location":"pmo/","page":"Polynomial Matrix Optimization","title":"Polynomial Matrix Optimization","text":"The following is a simple exmaple.","category":"page"},{"location":"pmo/","page":"Polynomial Matrix Optimization","title":"Polynomial Matrix Optimization","text":"using DynamicPolynomials\nusing TSSOS\n\n@polyvar x[1:5]\nF = [x[1]^4 x[1]^2 - x[2]*x[3] x[3]^2 - x[4]*x[5] x[1]*x[4] x[1]*x[5];\nx[1]^2 - x[2]*x[3] x[2]^4 x[2]^2 - x[3]*x[4] x[2]*x[4] x[2]*x[5];\nx[3]^2 - x[4]*x[5] x[2]^2 - x[3]*x[4] x[3]^4 x[4]^2 - x[1]*x[2] x[5]^2 - x[3]*x[5];\nx[1]*x[4] x[2]*x[4] x[4]^2 - x[1]*x[2] x[4]^4 x[4]^2 - x[1]*x[3];\nx[1]*x[5] x[2]*x[5] x[5]^2 - x[3]*x[5] x[4]^2 - x[1]*x[3] x[5]^4]\nG = Vector{Matrix{Polynomial{true, Int}}}(undef, 2)\nG[1] = [1 - x[1]^2 - x[2]^2 x[2]*x[3]; x[2]*x[3] 1 - x[3]^2]\nG[2] = [1 - x[4]^2 x[4]*x[5]; x[4]*x[5] 1 - x[5]^2]\n@time opt,data = tssos_first(F, G, x, 3, TS=\"MD\") # compute the first TS step of the TSSOS hierarchy\n@time opt,data = tssos_higher!(data, TS=\"MD\") # compute higher TS steps of the TSSOS hierarchy","category":"page"},{"location":"pmo/#Keyword-arguments","page":"Polynomial Matrix Optimization","title":"Keyword arguments","text":"","category":"section"},{"location":"pmo/","page":"Polynomial Matrix Optimization","title":"Polynomial Matrix Optimization","text":"Argument Description Default value\nCS Types of chordal extensions in exploiting correlative sparsity: \"MF\" (approximately smallest chordal extension), \"NC\" (not performing chordal extension), false (invalidating correlative sparsity exploitation) \"MF\"\ncliques Use customized variable cliques []\nTS Types of chordal extensions used in term sparsity iterations: \"block\"(maximal chordal extension), \"MD\" (approximately smallest chordal extension), false (invalidating term sparsity iterations) \"block\"\nQUIET Silence the output false\nsolve Solve the SDP relaxation true\nGram Output Gram matrices false\nMommat Output moment matrices false","category":"page"},{"location":"pmo/#References","page":"Polynomial Matrix Optimization","title":"References","text":"","category":"section"},{"location":"pmo/","page":"Polynomial Matrix Optimization","title":"Polynomial Matrix Optimization","text":"Sparse Polynomial Matrix Optimization, Jared Miller, Jie Wang, and Feng Guo, 2024.","category":"page"},{"location":"sorf/#Sum-Of-Rational-Functions-Optimization","page":"Sum-Of-Rational-Functions Optimization","title":"Sum-Of-Rational-Functions Optimization","text":"","category":"section"},{"location":"sorf/","page":"Sum-Of-Rational-Functions Optimization","title":"Sum-Of-Rational-Functions Optimization","text":"The sum-of-rational-functions optimization problem could be formulized as","category":"page"},{"location":"sorf/","page":"Sum-Of-Rational-Functions Optimization","title":"Sum-Of-Rational-Functions Optimization","text":"mathrminf_mathbfxinmathbfK sum_i=1^Nfracp_i(mathbfx)q_i(mathbfx)","category":"page"},{"location":"sorf/","page":"Sum-Of-Rational-Functions Optimization","title":"Sum-Of-Rational-Functions Optimization","text":"where p_iq_iinmathbbRmathbfx are polynomials and mathbfK is the basic semialgebraic set","category":"page"},{"location":"sorf/","page":"Sum-Of-Rational-Functions Optimization","title":"Sum-Of-Rational-Functions Optimization","text":"mathbfKcoloneqqlbrace mathbfxinmathbbR^n mid g_i(mathbfx)ge0 i=1ldotsm h_j(mathbfx)=0 j=1ldotsellrbrace","category":"page"},{"location":"sorf/","page":"Sum-Of-Rational-Functions Optimization","title":"Sum-Of-Rational-Functions Optimization","text":"for some polynomials g_ih_jinmathbbRmathbfx.","category":"page"},{"location":"sorf/","page":"Sum-Of-Rational-Functions Optimization","title":"Sum-Of-Rational-Functions Optimization","text":"The following is a simple example.","category":"page"},{"location":"sorf/","page":"Sum-Of-Rational-Functions Optimization","title":"Sum-Of-Rational-Functions Optimization","text":"@polyvar x y z\np = [x^2 + y^2 - y*z, y^2 + x^2*z, z^2 - x + y] # define the vector of denominators\nq = [1 + 2x^2 + y^2 + z^2, 1 + x^2 + 2y^2 + z^2, 1 + x^2 + y^2 + 2z^2] # define the vector of numerator\ng = [1 - x^2 - y^2 - z^2]\nd = 2 # set the relaxation order\nopt = SumOfRatios(p, q, g, [], [x;y;z], d, QUIET=true, SignSymmetry=true) # Without correlative sparsity\nopt = SparseSumOfRatios(p, q, g, [], [x;y;z], d, QUIET=true, SignSymmetry=true) # With correlative sparsity","category":"page"},{"location":"sorf/#Keyword-arguments","page":"Sum-Of-Rational-Functions Optimization","title":"Keyword arguments","text":"","category":"section"},{"location":"sorf/","page":"Sum-Of-Rational-Functions Optimization","title":"Sum-Of-Rational-Functions Optimization","text":"Argument Description Default value\nSignSymmetry Exploit sign symmetries true\nGroebnerbasis Work in the quotient ring by computing a GrÃ¶bner basis false","category":"page"},{"location":"sorf/#Methods","page":"Sum-Of-Rational-Functions Optimization","title":"Methods","text":"","category":"section"},{"location":"sorf/","page":"Sum-Of-Rational-Functions Optimization","title":"Sum-Of-Rational-Functions Optimization","text":"SumOfRatios\nSparseSumOfRatios","category":"page"},{"location":"sorf/#TSSOS.SumOfRatios","page":"Sum-Of-Rational-Functions Optimization","title":"TSSOS.SumOfRatios","text":"optimum = SumOfRatios(p, q, g, h, x, d; QUIET=false, SignSymmetry=true, Groebnerbasis=false)\n\nMinimizing the sum of ratios p1/q1 + ... + pN/qN on the set defined by g >= 0 and h == 0.\n\nInput arguments\n\np: vector of denominators\nq: vector of numerator\ng: inequality constraints\nh: equality constraints\nx: vector of variables\nd: relaxation order\n\nOutput arguments\n\nSignSymmetry: exploit sign symmetries or not (true, false)\nGroebnerbasis: exploit the quotient ring structure or not (true, false)\n\n\n\n\n\n","category":"function"},{"location":"sorf/#TSSOS.SparseSumOfRatios","page":"Sum-Of-Rational-Functions Optimization","title":"TSSOS.SparseSumOfRatios","text":"optimum = SparseSumOfRatios(p, q, g, h, x, d; QUIET=false, SignSymmetry=true, Groebnerbasis=false)\n\nMinimizing the sum of sparse ratios p1/q1 + ... + pN/qN on the set defined by g >= 0 and h == 0.\n\nInput arguments\n\np: vector of denominators\nq: vector of numerator\ng: inequality constraints\nh: equality constraints\nx: vector of variables\nd: relaxation order\n\nOutput arguments\n\nSignSymmetry: exploit sign symmetries or not (true, false)\nGroebnerbasis: exploit the quotient ring structure or not (true, false)\n\n\n\n\n\n","category":"function"},{"location":"sorf/#References","page":"Sum-Of-Rational-Functions Optimization","title":"References","text":"","category":"section"},{"location":"sorf/","page":"Sum-Of-Rational-Functions Optimization","title":"Sum-Of-Rational-Functions Optimization","text":"Exploiting Sign Symmetries in Minimizing Sums of Rational Functions, Feng Guo, Jie Wang, and Jianhao Zheng, 2024.","category":"page"},{"location":"opf/#AC-Optimal-Power-Flow","page":"AC Optimal Power Flow","title":"AC Optimal Power Flow","text":"","category":"section"},{"location":"sos/#Sum-Of-Squares-Optimization","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"","category":"section"},{"location":"sos/","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"A general sum-of-squares optimization (including polynomial optimization as a special case) problem takes the form:","category":"page"},{"location":"sos/","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"mathrminf_mathbfyinmathbbR^n mathbfc^intercalmathbfy","category":"page"},{"location":"sos/","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"mathrmst a_k0+y_1a_k1+cdots+y_na_kninmathrmSOS k=1ldotsm","category":"page"},{"location":"sos/","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"where mathbfcinmathbbR^n and a_kiinmathbbRmathbfx are polynomials. In TSSOS, SOS constraints could be handled with the routine add_psatz!:","category":"page"},{"location":"sos/","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"model,info = add_psatz!(model, nonneg, vars, ineq_cons, eq_cons, order, TS=\"block\", SO=1, Groebnerbasis=false)","category":"page"},{"location":"sos/","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"where nonneg is a nonnegative polynomial constrained to admit a Putinar's style SOS representation on the semialgebraic set defined by ineq_cons and eq_cons, and SO is the sparse order.","category":"page"},{"location":"sos/","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"The following is a simple exmaple.","category":"page"},{"location":"sos/","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"mathrmsup lambda","category":"page"},{"location":"sos/","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"mathrmst x_1^2 + x_1x_2 + x_2^2 + x_2x_3 + x_3^2 - lambda(x_1^2+x_2^2+x_3^2)=sigma+tau_1(x_1^2+x_2^2+y_1^2-1)+tau_2(x_2^2+x_3^2+y_2^2-1)","category":"page"},{"location":"sos/","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"sigmainmathrmSOSdeg(sigma)le2d tau_1tau_2inmathbbRmathbfxdeg(tau_1)deg(tau_2)le2d-2","category":"page"},{"location":"sos/","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"using JuMP\nusing MosekTools\nusing DynamicPolynomials\nusing MultivariatePolynomials\nusing TSSOS\n\n@polyvar x[1:3]\nf = x[1]^2 + x[1]*x[2] + x[2]^2 + x[2]*x[3] + x[3]^2\nd = 2 # set the relaxation order\n@polyvar y[1:2]\nh = [x[1]^2+x[2]^2+y[1]^2-1, x[2]^2+x[3]^2+y[2]^2-1]\nmodel = Model(optimizer_with_attributes(Mosek.Optimizer))\n@variable(model, lower)\nnonneg = f - lower*sum(x.^2)\nmodel,info = add_psatz!(model, nonneg, [x; y], [], h, d, TS=\"block\", Groebnerbasis=true)\n@objective(model, Max, lower)\noptimize!(model)","category":"page"},{"location":"sos/#Keyword-arguments","page":"Sum-Of-Squares Optimization","title":"Keyword arguments","text":"","category":"section"},{"location":"sos/","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"Argument Description Default value\nCS Types of chordal extensions in exploiting correlative sparsity: \"MF\" (approximately smallest chordal extension), \"NC\" (not performing chordal extension), false (invalidating correlative sparsity exploitation) \"MF\"\ncliques Use customized variable cliques []\nTS Types of chordal extensions used in term sparsity iterations: \"block\"(maximal chordal extension), \"signsymmetry\" (sign symmetries), \"MD\" (approximately smallest chordal extension), false (invalidating term sparsity iterations) \"block\"\nQUIET Silence the output false\nSO Specify the sparse order 1\nGroebnerbasis Work in the quotient ring by computing a GrÃ¶bner basis false","category":"page"},{"location":"sos/#Methods","page":"Sum-Of-Squares Optimization","title":"Methods","text":"","category":"section"},{"location":"sos/","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"add_psatz!\nadd_poly!\nadd_SOS!","category":"page"},{"location":"sos/#TSSOS.add_psatz!","page":"Sum-Of-Squares Optimization","title":"TSSOS.add_psatz!","text":"model,info = add_psatz!(model, nonneg, vars, ineq_cons, eq_cons, order; CS=false, cliques=[], TS=\"block\", \nSO=1, Groebnerbasis=false, QUIET=false, constrs=nothing)\n\nAdd a Putinar's style SOS representation of the polynomial nonneg to the JuMP model.\n\nInput arguments\n\nmodel: a JuMP optimization model\nnonneg: a nonnegative polynomial constrained to be a Putinar's style SOS on a semialgebraic set\nvars: the set of POP variables\nineq_cons: inequality constraints\neq_cons: equality constraints\norder: relaxation order\nCS: method of chordal extension for correlative sparsity (\"MF\", \"MD\", \"NC\", false)\ncliques: the set of cliques used in correlative sparsity\nTS: type of term sparsity (\"block\", \"signsymmetry\", \"MD\", \"MF\", false)\nSO: sparse order\nGroebnerbasis: exploit the quotient ring structure or not (true, false)\nQUIET: run in the quiet mode (true, false)\nconstrs: the constraint name used in the JuMP model\n\nOutput arguments\n\nmodel: the modified JuMP model\ninfo: other auxiliary data\n\n\n\n\n\n","category":"function"},{"location":"sos/#TSSOS.add_poly!","page":"Sum-Of-Squares Optimization","title":"TSSOS.add_poly!","text":"p,coe,mon = add_poly!(model, vars, degree)\n\nGenerate an unknown polynomial of given degree whose coefficients are from the JuMP model.\n\nInput arguments\n\nmodel: a JuMP optimization model\nvars: set of variables\ndegree: degree of the polynomial\n\nOutput arguments\n\np: the polynomial \ncoe: coefficients of the polynomial \nmon: monomials of the polynomial \n\n\n\n\n\n","category":"function"},{"location":"sos/#TSSOS.add_SOS!","page":"Sum-Of-Squares Optimization","title":"TSSOS.add_SOS!","text":"sos = add_SOS!(model, vars, d)\n\nGenerate an SOS polynomial of degree 2d whose coefficients are from the JuMP model.\n\nInput arguments\n\nmodel: a JuMP optimization model\nvars: set of variables\nd: half degree of the SOS polynomial\n\nOutput arguments\n\nsos: the sos polynomial \n\n\n\n\n\n","category":"function"},{"location":"technique/#Techniques","page":"Techniques","title":"Techniques","text":"","category":"section"},{"location":"technique/#Homogenization","page":"Techniques","title":"Homogenization","text":"","category":"section"},{"location":"technique/#Christoffel-Darboux-Kernels","page":"Techniques","title":"Christoffel-Darboux Kernels","text":"","category":"section"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"Here is the list of functions implemented in TSSOS that are based on computing Christiffel-Darboux kernels associated to a given POP:","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"- run_H1 , run_H1CS , run_H2 , run_H2CS,\n- construct_CDK, construct_marginal_CDK, construct_CDK_cs, construct_marginal_CDK_cs","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"We provide below some illustrations.  Let us consider the following POP (Example 3.2 from [Sparse polynomial optimization: theory and practice]):","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"n = 6\n@polyvar x[1:n]\nf = x[2]*x[5] + x[3]*x[6] - x[2]*x[3] - x[5]*x[6] + x[1]*(-x[1] + x[2] + x[3] - x[4] + x[5] + x[6])\ng = [(6.36 - x[i]) * (x[i] - 4) for i in 1:6]\npop = [f, g...]\nd = 1","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"We can solve the problem using the dense moment-SOS hierarchy:","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"opt, sol, data = cs_tssos_first(pop, x, d, TS=false, CS=false, solution=true)","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"Afterwards, one can try strenghtening the bound via H1:","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"N = 5\neps = 0.05\ndc = 1\ngap_tol = 0.1\nresultH1 = run_H1(pop,x,d,dc,N,eps,gap_tol)","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"or via H2","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"dc = 1\nlocal_sol = sol\ntau = 1.1\nresultH2 = run_H2(pop,x,d,dc,local_sol,tau)","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"Alternatively, the problem can be solved using the correlatively sparse moment-SOS hierarchy:","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"opt, sol, data = cs_tssos_first(pop, x, d, TS=false, solution=true)","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"Afterwards, the bound can be strengthened either via H1CS:","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"N = 5\neps = 0.05\ndc = 1\ngap_tol = 0.1\nresultH1CS = run_H1CS(pop,x,d,dc,N,eps,gap_tol)","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"or via H2CS","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"dc = 1\nlocal_sol = sol\ntau = 1.1\nresultH2CS = run_H2CS(pop,x,d,dc,local_sol,tau)","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"Moreover, here is how different Christoffel polynomials can be constructed using the output of the: ","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"dense Moment-SOS hierarchy of order 2","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"d = 2\nopt, sol, data = cs_tssos_first(pop, x, d, TS=false, CS=false, solution=true, Mommat=true);\n\nk = 4\ndc = 1\nCDK_order1 = construct_CDK(x, dc, data.moment[1])  # Constructs multivariate Christoffel polynomial of order dc=1 (quadratic CDK)\nCDK_4_order1 = construct_marginal_CDK(x, k, dc, data.moment[1])  # Constructs marginal Christoffel polynomial, associated to x_4, of order dc=1 \n\ndc = 2\nCDK_order2 = construct_CDK(x, dc, data.moment[1])  # Construct multivariate Christoffel polynomial of order dc=2 (quartic CDK)\nCDK_4_order2 = construct_marginal_CDK(x, k, dc, data.moment[1])  # Constructs marginal Christoffel polynomial, associated to x_4, of order dc=2 \n","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"sparse Moment-SOS hierarchy of order 2","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"d = 2\nopt, sol, data = cs_tssos_first(pop, x, d, TS=false, solution=true, Mommat=true);\n\nk = 4\ndc = 1\nCDK_sparse_order1 = construct_CDK_cs(x, dc, data.moment, data.cliques)  # Constructs multivariate Christoffel polynomial of order dc=1 for each identified clique.\nCDK_sparse_4_order1 = construct_marginal_CDK_cs(x, k, dc, data.moment, data.cliques)  # Constructs marginal Christoffel polynomial, associated to x_4, of order dc=1 \n\ndc = 2\nCDK_sparse_order2 = construct_CDK_cs(x, dc, data.moment, data.cliques)  # Constructs multivariate Christoffel polynomial of order dc=2 for each identified clique.\nCDK_sparse_4_order2 = construct_marginal_CDK_cs(x, k, dc, data.moment, data.cliques)  # Constructs marginal Christoffel polynomial, associated to x_4, of order dc=2\n","category":"page"},{"location":"technique/#Tips-for-modelling-polynomial-optimization-problems","page":"Techniques","title":"Tips for modelling polynomial optimization problems","text":"","category":"section"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"When possible, explictly include a sphere/ball constraint (or multi-sphere/multi-ball constraints).\nWhen the feasible set is unbounded, try the homogenization technique introduced in Homogenization for polynomial optimization with unbounded sets.\nScale the coefficients of the polynomial optimization problem to -1 1.\nScale the variables so that they take values in -1 1 or 0 1.\nTry to include more (redundant) inequality constraints.","category":"page"},{"location":"structure/#Structures","page":"Structures","title":"Structures","text":"","category":"section"},{"location":"#TSSOS","page":"Home","title":"TSSOS","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"TSSOS aims to provide a user-friendly and efficient tool for solving optimization problems with polynomials, which is based on the structured moment-SOS hierarchy.","category":"page"},{"location":"#Authors","page":"Home","title":"Authors","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Jie Wang, Academy of Mathematics and Systems Science, Chinese Academy of Sciences.\nVictor Magron, Laboratoire d'Architecture et Analyse des SystÃ¨mes, CNRS.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"TSSOS could be installed by running","category":"page"},{"location":"","page":"Home","title":"Home","text":"pkg> add https://github.com/wangjie212/TSSOS","category":"page"},{"location":"#Related-packages","page":"Home","title":"Related packages","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"DynamicPolynomials: Polynomial definition\nMultivariatePolynomials: Polynomial manipulation\nNCTSSOS: Noncommutative polynomial optimization\nChordalGraph: Chordal graphs and chordal extentions\nSparseJSR: Computing joint spetral radius","category":"page"},{"location":"#References","page":"Home","title":"References","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"TSSOS: A Moment-SOS hierarchy that exploits term sparsity, Jie Wang, Victor Magron, and Jean B. Lasserre, 2021.\nChordal-TSSOS: a moment-SOS hierarchy that exploits term sparsity with chordal extension, Jie Wang, Victor Magron, and Jean B. Lasserre, 2021.\nCS-TSSOS: Correlative and term sparsity for large-scale polynomial optimization, Jie Wang, Victor Magron, Jean B. Lasserre, and Ngoc H. A. Mai, 2022.\nTSSOS: a Julia library to exploit sparsity for large-scale polynomial optimization, Victor Magron and Jie Wang, 2021.","category":"page"},{"location":"cpop/#Complex-Polynomial-Optimization","page":"Complex Polynomial Optimization","title":"Complex Polynomial Optimization","text":"","category":"section"},{"location":"cpop/","page":"Complex Polynomial Optimization","title":"Complex Polynomial Optimization","text":"A general complex polynomial optimization problem could be formulized as","category":"page"},{"location":"cpop/","page":"Complex Polynomial Optimization","title":"Complex Polynomial Optimization","text":"mathrminf_mathbfzinmathbfK f(mathbfzbarmathbfz)","category":"page"},{"location":"cpop/","page":"Complex Polynomial Optimization","title":"Complex Polynomial Optimization","text":"with","category":"page"},{"location":"cpop/","page":"Complex Polynomial Optimization","title":"Complex Polynomial Optimization","text":"mathbfKcoloneqqlbrace mathbfzinmathbbC^n mid g_i(mathbfzbarmathbfz)ge0 i=1ldotsm h_j(mathbfzbarmathbfz)=0 j=1ldotsellrbrace","category":"page"},{"location":"cpop/","page":"Complex Polynomial Optimization","title":"Complex Polynomial Optimization","text":"where barmathbfz stands for the conjugate of mathbfz=(z_1ldotsz_n), and f g_i i=1ldotsm h_j j=1ldotsell are real-valued complex polynomials satisfying barf=f and barg_i=g_i, barh_j=h_j.","category":"page"},{"location":"cpop/","page":"Complex Polynomial Optimization","title":"Complex Polynomial Optimization","text":"In TSSOS, we use x_i to represent the complex variable z_i and use x_n+i to represent its conjugate barz_i. Let us consider the following example:","category":"page"},{"location":"cpop/","page":"Complex Polynomial Optimization","title":"Complex Polynomial Optimization","text":"mathrminf 3-z_1^2-05mathbfiz_1barz_2^2+05mathbfiz_2^2barz_1","category":"page"},{"location":"cpop/","page":"Complex Polynomial Optimization","title":"Complex Polynomial Optimization","text":"mathrmst z_2+barz_2ge0z_1^2-025z_1^2-025barz_1^2=1z_1^2+z_2^2=3mathbfiz_2-mathbfibarz_2=0","category":"page"},{"location":"cpop/","page":"Complex Polynomial Optimization","title":"Complex Polynomial Optimization","text":"which is represented in TSSOS as","category":"page"},{"location":"cpop/","page":"Complex Polynomial Optimization","title":"Complex Polynomial Optimization","text":"mathrminf 3-x_1x_3-05mathbfix_1x_4^2+05mathbfix_2^2x_3","category":"page"},{"location":"cpop/","page":"Complex Polynomial Optimization","title":"Complex Polynomial Optimization","text":"mathrmst x_2+x_4ge0x_1x_3-025x_1^2-025x_3^2=1x_1x_3+x_2x_4=3mathbfix_2-mathbfix_4=0","category":"page"},{"location":"cpop/","page":"Complex Polynomial Optimization","title":"Complex Polynomial Optimization","text":"using DynamicPolynomials\nn = 2 # set the number of complex variables\n@polyvar x[1:2n]\nf = 3 - x[1]*x[3] - 0.5im*x[1]*x[4]^2 + 0.5im*x[2]^2*x[3]\ng1 = x[2] + x[4]\nh1 = x[1]*x[3] - 0.25*x[1]^2 - 0.25 x[3]^2 - 1\nh2 = x[1]*x[3] + x[2]*x[4] - 3\nh3 = im*x[2] - im*x[4]\npop = [f, g, h1, h2, h3]\norder = 2 # set the relaxation order\nopt,sol,data = cs_tssos_first(pop, x, n, order, numeq=3) # compute the first TS step of the CS-TSSOS hierarchy\nopt,sol,data = cs_tssos_higher!(data) # compute higher TS steps of the CS-TSSOS hierarchy","category":"page"},{"location":"cpop/#Keyword-arguments","page":"Complex Polynomial Optimization","title":"Keyword arguments","text":"","category":"section"},{"location":"cpop/","page":"Complex Polynomial Optimization","title":"Complex Polynomial Optimization","text":"Argument Description Default value\nnb Specify the first nb complex variables to be of unit norm 0\nnumeq Specify the last numeq constraints to be equality constraints 0\nCS Types of chordal extensions in exploiting correlative sparsity: \"MF\" (approximately smallest chordal extension), \"NC\" (not performing chordal extension), false (invalidating correlative sparsity exploitation) \"MF\"\ncliques Use customized variable cliques []\nTS Types of chordal extensions used in term sparsity iterations: \"block\"(maximal chordal extension), \"MD\" (approximately smallest chordal extension), false (invalidating term sparsity iterations) \"block\"\nipart Use complex moment matrices true\nbalanced Impose the balanced structure false\nnormality Impose normality condtions of order normality 0\nNormalSparse Exploit sparsity when imposing normality conditions false\nmerge Merge overlapping PSD blocks false\nmd Parameter for tunning the merging strength 3\nMomentOne add a first-order moment PSD constraint for each variable clique false\nsolver Specify an SDP solver: \"Mosek\" or \"COSMO\" \"Mosek\"\ncosmo_setting Parameters for the COSMO solver: cosmopara(epsabs, epsrel, maxiter, time_limit) cosmo_para(1e-5, 1e-5, 1e4, 0)\nmosek_setting Parameters for the Mosek solver: cosmopara(tolpfeas, toldfeas, tolrelgap, timelimit, numthreads) mosek_para(1e-8, 1e-8, 1e-8, -1, 0)\nQUIET Silence the output false\nsolve Solve the SDP relaxation true\ndualize Solve the dual SDP problem false\nGram Output Gram matrices false\nMommat Output moment matrices false\nsolution Extract an approximately optimal solution false\ntol Tolerance for certifying global optimality 1e-4","category":"page"},{"location":"cpop/#References","page":"Complex Polynomial Optimization","title":"References","text":"","category":"section"},{"location":"cpop/","page":"Complex Polynomial Optimization","title":"Complex Polynomial Optimization","text":"Exploiting Sparsity in Complex Polynomial Optimization, Jie Wang and Victor Magron, 2021.","category":"page"},{"location":"pop/#Polynomial-Optimization","page":"Polynomial Optimization","title":"Polynomial Optimization","text":"","category":"section"},{"location":"pop/","page":"Polynomial Optimization","title":"Polynomial Optimization","text":"Polynomial optimization concerns minimizing a polynomial subject to a tuple of polynomial inequality constraints and equality constraints, which in general takes the form:","category":"page"},{"location":"pop/","page":"Polynomial Optimization","title":"Polynomial Optimization","text":"mathrminf_mathbfxinmathbbR^n f(mathbfx) text st  g_1(mathbfx)ge0ldotsg_m(mathbfx)ge0h_1(mathbfx)=0ldotsh_ell(mathbfx)=0","category":"page"},{"location":"pop/","page":"Polynomial Optimization","title":"Polynomial Optimization","text":"where fg_1ldotsg_mh_1ldotsh_ellinmathbbRmathbfx are polynomials in variables mathbfx.","category":"page"},{"location":"pop/","page":"Polynomial Optimization","title":"Polynomial Optimization","text":"To illustrate how to solve a polynomial optimization problem with TSSOS, let us consider f=1+x_1^4+x_2^4+x_3^4+x_1x_2x_3+x_2 and g=1-x_1^2-2x_2^2 h=x_2^2+x_3^2-1.","category":"page"},{"location":"pop/","page":"Polynomial Optimization","title":"Polynomial Optimization","text":"@polyvar x[1:3]\nf = 1 + x[1]^4 + x[2]^4 + x[3]^4 + x[1]*x[2]*x[3] + x[2]\ng = 1 - x[1]^2 - 2*x[2]^2\nh = x[2]^2 + x[3]^2 - 1\npop = [f, g, h]\nd = 2 # set the relaxation order\nopt,sol,data = tssos_first(pop, x, d, numeq=1, TS=\"MD\") # compute the first TS step of the TSSOS hierarchy\nopt,sol,data = tssos_higher!(data, TS=\"MD\") # compute higher TS steps of the TSSOS hierarchy","category":"page"},{"location":"pop/#Keyword-arguments","page":"Polynomial Optimization","title":"Keyword arguments","text":"","category":"section"},{"location":"pop/","page":"Polynomial Optimization","title":"Polynomial Optimization","text":"Argument Description Default value\nnb Specify the first nb variables to be pm1 binary variables 0\nnumeq Specify the last numeq constraints to be equality constraints 0\nquotient Work in the quotient ring by computing a GrÃ¶bner basis true\nbasis Use customized monomial bases []\nreducebasis Reduce the monomial bases false\nTS Types of chordal extensions used in term sparsity iterations: \"block\"(maximal chordal extension), \"signsymmetry\" (sign symmetries), \"MD\" (approximately smallest chordal extension), false (invalidating term sparsity exploitation) \"block\"\nnormality Impose normality condtions false\nNormalSparse Exploit sparsity when imposing normality conditions false\nmerge Merge overlapping PSD blocks false\nmd Parameter for tunning the merging strength 3\nMomentOne add a first-order moment PSD constraint false\nsolver Specify an SDP solver: \"Mosek\" or \"COSMO\" \"Mosek\"\ncosmo_setting Parameters for the COSMO solver: cosmopara(epsabs, epsrel, maxiter, time_limit) cosmo_para(1e-5, 1e-5, 1e4, 0)\nmosek_setting Parameters for the Mosek solver: cosmopara(tolpfeas, toldfeas, tolrelgap, timelimit, numthreads) mosek_para(1e-8, 1e-8, 1e-8, -1, 0)\nQUIET Silence the output false\nsolve Solve the SDP relaxation true\ndualize Solve the dual SDP problem false\nGram Output Gram matrices false\nsolution Extract optimal solutions false\ntol Tolerance for certifying global optimality 1e-4","category":"page"},{"location":"pop/#Correlative-sparsity","page":"Polynomial Optimization","title":"Correlative sparsity","text":"","category":"section"},{"location":"pop/","page":"Polynomial Optimization","title":"Polynomial Optimization","text":"The following is an example where one exploits correlative sparsity and term sparsity simultaneously.","category":"page"},{"location":"pop/","page":"Polynomial Optimization","title":"Polynomial Optimization","text":"using DynamicPolynomials\nn = 6\n@polyvar x[1:n]\nf = 1 + sum(x.^4) + x[1]*x[2]*x[3] + x[3]*x[4]*x[5] + x[3]*x[4]*x[6]+x[3]*x[5]*x[6] + x[4]*x[5]*x[6]\npop = [f, 1-sum(x[1:3].^2), 1-sum(x[3:6].^2)]\norder = 2 # set the relaxation order\nopt,sol,data = cs_tssos_first(pop, x, order, numeq=0, TS=\"MD\") # compute the first TS step of the CS-TSSOS hierarchy\nopt,sol,data = cs_tssos_higher!(data, TS=\"MD\") # compute higher TS steps of the CS-TSSOS hierarchy","category":"page"},{"location":"pop/#Keyword-arguments-2","page":"Polynomial Optimization","title":"Keyword arguments","text":"","category":"section"},{"location":"pop/","page":"Polynomial Optimization","title":"Polynomial Optimization","text":"Argument Description Default value\nnb Specify the first nb variables to be pm1 binary variables 0\nnumeq Specify the last numeq constraints to be equality constraints 0\nCS Types of chordal extensions in exploiting correlative sparsity: \"MF\" (approximately smallest chordal extension), \"NC\" (not performing chordal extension), false (invalidating correlative sparsity exploitation) \"MF\"\nbasis Use customized monomial bases []\nhbasis Use customized monomial bases associated with equality constraints []\ncliques Use customized variable cliques []\nTS Types of chordal extensions used in term sparsity iterations: \"block\"(maximal chordal extension), \"signsymmetry\" (sign symmetries), \"MD\" (approximately smallest chordal extension), false (invalidating term sparsity iterations) \"block\"\nnormality Impose normality condtions false\nNormalSparse Exploit sparsity when imposing normality conditions false\nmerge Merge overlapping PSD blocks false\nmd Parameter for tunning the merging strength 3\nMomentOne add a first-order moment PSD constraint for each variable clique false\nsolver Specify an SDP solver: \"Mosek\" or \"COSMO\" \"Mosek\"\ncosmo_setting Parameters for the COSMO solver: cosmopara(epsabs, epsrel, maxiter, time_limit) cosmo_para(1e-5, 1e-5, 1e4, 0)\nmosek_setting Parameters for the Mosek solver: cosmopara(tolpfeas, toldfeas, tolrelgap, timelimit, numthreads) mosek_para(1e-8, 1e-8, 1e-8, -1, 0)\nQUIET Silence the output false\nsolve Solve the SDP relaxation true\ndualize Solve the dual SDP problem false\nGram Output Gram matrices false\nMommat Output moment matrices false\nsolution Extract an approximately optimal solution false\ntol Tolerance for certifying global optimality 1e-4","category":"page"},{"location":"pop/#Compute-a-locally-optimal-solution","page":"Polynomial Optimization","title":"Compute a locally optimal solution","text":"","category":"section"},{"location":"pop/","page":"Polynomial Optimization","title":"Polynomial Optimization","text":"Moment-SOS relaxations provide lower bounds on the optimum of the polynomial optimization problem. As the complementary side, one could compute a locally optimal solution which provides an upper bound on the optimum of the polynomial optimization problem. The upper bound is useful in evaluating the quality (tightness) of those lower bounds provided by moment-SOS relaxations. In TSSOS, for a given polynomial optimization problem, a locally optimal solution could be obtained via the nonlinear programming solver Ipopt:","category":"page"},{"location":"pop/","page":"Polynomial Optimization","title":"Polynomial Optimization","text":"obj,sol,status = local_solution(data.n, data.m, data.supp, data.coe, numeq=data.numeq, startpoint=rand(data.n))","category":"page"},{"location":"pop/#Methods","page":"Polynomial Optimization","title":"Methods","text":"","category":"section"},{"location":"pop/","page":"Polynomial Optimization","title":"Polynomial Optimization","text":"tssos_first\ntssos_higher!\ncs_tssos_first\ncs_tssos_higher!\nlocal_solution\nrefine_sol\nextract_solutions\nextract_solutions_robust","category":"page"},{"location":"pop/#TSSOS.tssos_first","page":"Polynomial Optimization","title":"TSSOS.tssos_first","text":"opt,sol,data = tssos_first(f, x; nb=0, newton=true, reducebasis=false, TS=\"block\", merge=false,\nmd=3, feasible=false, solver=\"Mosek\", QUIET=false, solve=true, MomentOne=false, Gram=false, solution=false, tol=1e-4)\n\nCompute the first TS step of the TSSOS hierarchy for unconstrained polynomial optimization. If newton=true, then compute a monomial basis by the Newton polytope method. If reducebasis=true, then remove monomials from the monomial basis by diagonal inconsistency. If TS=\"block\", use maximal chordal extensions; if TS=\"MD\", use approximately smallest chordal extensions.  If merge=true, perform the PSD block merging.  If feasible=true, then solve the feasibility problem. If solve=false, then do not solve the SDP. If Gram=true, then output the Gram matrix. If MomentOne=true, add an extra first-order moment PSD constraint to the moment relaxation.\n\nInput arguments\n\nf: objective\nx: POP variables\nnb: number of binary variables in x\nTS: type of term sparsity (\"block\", \"signsymmetry\", \"MD\", \"MF\", false)\nmd: tunable parameter for merging blocks\nQUIET: run in the quiet mode (true, false)\ntol: relative tolerance to certify global optimality\n\nOutput arguments\n\nopt: optimum\nsol: (near) optimal solution (if solution=true)\ndata: other auxiliary data \n\n\n\n\n\nopt,sol,data = tssos_first(pop, x, d; nb=0, numeq=0, quotient=true, basis=[],\nreducebasis=false, TS=\"block\", merge=false, md=3, solver=\"Mosek\", QUIET=false, solve=true,\nMomentOne=false, Gram=false, solution=false, tol=1e-4)\n\nCompute the first TS step of the TSSOS hierarchy for constrained polynomial optimization. If quotient=true, then exploit the quotient ring structure defined by the equality constraints. If merge=true, perform the PSD block merging.  If solve=false, then do not solve the SDP. If Gram=true, then output the Gram matrix. If MomentOne=true, add an extra first-order moment PSD constraint to the moment relaxation.\n\nInput arguments\n\npop: vector of the objective, inequality constraints, and equality constraints\nx: POP variables\nd: relaxation order\nnb: number of binary variables in x\nnumeq: number of equality constraints\nTS: type of term sparsity (\"block\", \"signsymmetry\", \"MD\", \"MF\", false)\nmd: tunable parameter for merging blocks\nnormality: impose the normality condtions (true, false)\nQUIET: run in the quiet mode (true, false)\ntol: relative tolerance to certify global optimality\n\nOutput arguments\n\nopt: optimum\nsol: (near) optimal solution (if solution=true)\ndata: other auxiliary data \n\n\n\n\n\n","category":"function"},{"location":"pop/#TSSOS.tssos_higher!","page":"Polynomial Optimization","title":"TSSOS.tssos_higher!","text":"opt,sol,data = tssos_higher!(data; TS=\"block\", merge=false, md=3, QUIET=false, solve=true,\nMomentOne=false, solution=false, tol=1e-4)\n\nCompute higher TS steps of the TSSOS hierarchy.\n\n\n\n\n\n","category":"function"},{"location":"pop/#TSSOS.cs_tssos_first","page":"Polynomial Optimization","title":"TSSOS.cs_tssos_first","text":"opt,sol,data = cs_tssos_first(pop, x, d; nb=0, numeq=0, CS=\"MF\", cliques=[], TS=\"block\", merge=false, md=3, solver=\"Mosek\", QUIET=false, solve=true, solution=false,\nGram=false, MomentOne=false, Mommat=false, tol=1e-4)\n\nCompute the first TS step of the CS-TSSOS hierarchy for constrained polynomial optimization. If merge=true, perform the PSD block merging.  If solve=false, then do not solve the SDP. If Gram=true, then output the Gram matrix. If Mommat=true, then output the moment matrix. If MomentOne=true, add an extra first-order moment PSD constraint to the moment relaxation.\n\nInput arguments\n\npop: vector of the objective, inequality constraints, and equality constraints\nx: POP variables\nd: relaxation order\nnb: number of binary variables in x\nnumeq: number of equality constraints\nCS: method of chordal extension for correlative sparsity (\"MF\", \"MD\", \"NC\", false)\ncliques: the set of cliques used in correlative sparsity\nTS: type of term sparsity (\"block\", \"signsymmetry\", \"MD\", \"MF\", false)\nmd: tunable parameter for merging blocks\nnormality: impose the normality condtions (true, false)\nQUIET: run in the quiet mode (true, false)\ntol: relative tolerance to certify global optimality\n\nOutput arguments\n\nopt: optimum\nsol: (near) optimal solution (if solution=true)\ndata: other auxiliary data \n\n\n\n\n\nopt,sol,data = cs_tssos_first(supp::Vector{Vector{Vector{UInt16}}}, coe, n, d; nb=0, numeq=0, CS=\"MF\", cliques=[], TS=\"block\", \nmerge=false, md=3, QUIET=false, solver=\"Mosek\", solve=true, solution=false, Gram=false, MomentOne=false, Mommat=false, tol=1e-4)\n\nCompute the first TS step of the CS-TSSOS hierarchy for constrained polynomial optimization.  Here the polynomial optimization problem is defined by supp and coe, corresponding to the supports and coeffients of pop respectively.\n\n\n\n\n\nopt,sol,data = cs_tssos_first(pop, z, n, d; nb=0, numeq=0, CS=\"MF\", cliques=[], TS=\"block\", ipart=true, merge=false, md=3, \nsolver=\"Mosek\", QUIET=false, solve=true, Gram=false, Mommat=false, MomentOne=false)\n\nCompute the first TS step of the CS-TSSOS hierarchy for constrained complex polynomial optimization.  If merge=true, perform the PSD block merging.  If ipart=false, then use the real moment-HSOS hierarchy. If solve=false, then do not solve the SDP. If Gram=true, then output the Gram matrix. If Mommat=true, then output the moment matrix. If MomentOne=true, add an extra first-order moment PSD constraint to the moment relaxation.\n\nInput arguments\n\npop: vector of the objective, inequality constraints, and equality constraints\nz: CPOP variables and their conjugate\nn: number of CPOP variables\nd: relaxation order\nnb: number of unit-norm variables in x\nnumeq: number of equality constraints\nCS: method of chordal extension for correlative sparsity (\"MF\", \"MD\", false)\ncliques: the set of cliques used in correlative sparsity\nTS: type of term sparsity (\"block\", \"MD\", \"MF\", false)\nmd: tunable parameter for merging blocks\nnormality: normal order\nQUIET: run in the quiet mode (true, false)\n\nOutput arguments\n\nopt: optimum\nsol: (near) optimal solution (if solution=true)\ndata: other auxiliary data \n\n\n\n\n\nopt,sol,data = cs_tssos_first(supp::Vector{Vector{Vector{Vector{UInt16}}}}, coe::Vector{Vector{ComplexF64}},\nn, d; nb=0, numeq=0, CS=\"MF\", cliques=[], TS=\"block\", ipart=true, merge=false, md=3, solver=\"Mosek\",\nQUIET=false, solve=true, Gram=false, Mommat=false, MomentOne=false)\n\nCompute the first TS step of the CS-TSSOS hierarchy for constrained complex polynomial optimization.  Here the complex polynomial optimization problem is defined by supp and coe, corresponding to the supports and coeffients of pop respectively.\n\n\n\n\n\n","category":"function"},{"location":"pop/#TSSOS.cs_tssos_higher!","page":"Polynomial Optimization","title":"TSSOS.cs_tssos_higher!","text":"opt,sol,data = cs_tssos_higher!(data; TS=\"block\", merge=false, md=3, QUIET=false, solve=true,\nsolution=false, Gram=false, Mommat=false, MomentOne=false)\n\nCompute higher TS steps of the CS-TSSOS hierarchy.\n\n\n\n\n\nopt,sol,data = cs_tssos_higher!(data; TS=\"block\", merge=false, md=3, QUIET=false, solve=true,\nsolution=false, Gram=false, Mommat=false, MomentOne=false)\n\nCompute higher TS steps of the CS-TSSOS hierarchy.\n\n\n\n\n\n","category":"function"},{"location":"pop/#TSSOS.local_solution","page":"Polynomial Optimization","title":"TSSOS.local_solution","text":"obj,sol,status = local_solution(n, m, supp::Vector{Union{Vector{Vector{UInt16}}, Array{UInt8, 2}}}, coe; nb=0, numeq=0,\nstartpoint=[], QUIET=false)\n\nCompute a local solution by a local solver.\n\nInput arguments\n\nn: number of POP variables\nm: number of POP constraints\nsupp: supports of the POP\ncoe: coefficients of the POP\nnb: number of binary variables\nnumeq: number of equality constraints\nstartpoint: provide a start point\nQUIET: run in the quiet mode or not (true, false)\n\nOutput arguments\n\nobj: local optimum\nsol: local solution\nstatus: solver termination status\n\n\n\n\n\n","category":"function"},{"location":"pop/#TSSOS.refine_sol","page":"Polynomial Optimization","title":"TSSOS.refine_sol","text":"ref_sol,flag = refine_sol(opt, sol, data, QUIET=false, tol=1e-4)\n\nRefine the obtained solution by a local solver. Return the refined solution, and flag=0 if global optimality is certified, flag=1 otherwise.\n\n\n\n\n\n","category":"function"},{"location":"pop/#TSSOS.extract_solutions","page":"Polynomial Optimization","title":"TSSOS.extract_solutions","text":"sol = extract_solutions(pop, x, d, opt, moment; numeq=0, nb=0, tol=1e-4)\n\nExtract a set of solutions for the polynomial optimization problem.\n\nInput arguments\n\npop: polynomial optimization problem\nx: set of variables\nd: relaxation order\nopt: optimum\nmoment: moment matrix\nnumeq: number of equality constraints\nnb: number of binary variables\ntol: tolerance to obtain the column echelon form\n\nOutput arguments\n\nsol: a set of solutions\n\n\n\n\n\n","category":"function"},{"location":"pop/#TSSOS.extract_solutions_robust","page":"Polynomial Optimization","title":"TSSOS.extract_solutions_robust","text":"sol = extract_solutions_robust(n, d, moment; tol=1e-2)\n\nExtract a set of solutions for the polynomial optimization problem.\n\nInput arguments\n\nn: number of variables\nd: relaxation order\nmoment: moment matrix\ntol: tolerance to obtain the column echelon form\n\nOutput arguments\n\nsol: a set of solutions\n\n\n\n\n\n","category":"function"}]
}
