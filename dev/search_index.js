var documenterSearchIndex = {"docs":
[{"location":"pmo/#Polynomial-Matrix-Optimization","page":"Polynomial Matrix Optimization","title":"Polynomial Matrix Optimization","text":"","category":"section"},{"location":"pmo/","page":"Polynomial Matrix Optimization","title":"Polynomial Matrix Optimization","text":"The polynomial matrix optimization problem aims to minimize the smallest eigenvalue of a polynomial matrix subject to a tuple of polynomial matrix inequalties (PMIs), which could be formulized as","category":"page"},{"location":"pmo/","page":"Polynomial Matrix Optimization","title":"Polynomial Matrix Optimization","text":"mathrminf_mathbfxinmathbfK lambda_mathrmmin(F(mathbfx))","category":"page"},{"location":"pmo/","page":"Polynomial Matrix Optimization","title":"Polynomial Matrix Optimization","text":"where FinmathbbSmathbfx^p is a ptimes p symmetric polynomial matrix and mathbfK is the basic semialgebraic set","category":"page"},{"location":"pmo/","page":"Polynomial Matrix Optimization","title":"Polynomial Matrix Optimization","text":"mathbfKcoloneqqlbrace mathbfxinmathbbR^n mid G_j(mathbfx)succeq0 j=1ldotsmrbrace","category":"page"},{"location":"pmo/","page":"Polynomial Matrix Optimization","title":"Polynomial Matrix Optimization","text":"for some symmetric polynomial matrices G_jinmathbbSmathbfx^q_j j=1ldotsm. Note that when p=1, lambda_min(F(mathbfx))=F(mathbfx). More generally, one may consider","category":"page"},{"location":"pmo/","page":"Polynomial Matrix Optimization","title":"Polynomial Matrix Optimization","text":"mathrminf_mathbfyinmathbbR^t mathbfc^intercalmathbfy","category":"page"},{"location":"pmo/","page":"Polynomial Matrix Optimization","title":"Polynomial Matrix Optimization","text":"mathrmst F_0(mathbfx)+y_1F_1(mathbfx)+cdots+y_tF_t(mathbfx)succeq0 textrm on  K","category":"page"},{"location":"pmo/","page":"Polynomial Matrix Optimization","title":"Polynomial Matrix Optimization","text":"where F_iinmathbbSmathbfx^p i=01ldotst are a tuple of symmetric polynomial matrices.","category":"page"},{"location":"pmo/","page":"Polynomial Matrix Optimization","title":"Polynomial Matrix Optimization","text":"The following is a simple exmaple.","category":"page"},{"location":"pmo/","page":"Polynomial Matrix Optimization","title":"Polynomial Matrix Optimization","text":"using DynamicPolynomials\nusing TSSOS\n\n@polyvar x[1:5]\nF = [x[1]^4 x[1]^2 - x[2]*x[3] x[3]^2 - x[4]*x[5] x[1]*x[4] x[1]*x[5];\nx[1]^2 - x[2]*x[3] x[2]^4 x[2]^2 - x[3]*x[4] x[2]*x[4] x[2]*x[5];\nx[3]^2 - x[4]*x[5] x[2]^2 - x[3]*x[4] x[3]^4 x[4]^2 - x[1]*x[2] x[5]^2 - x[3]*x[5];\nx[1]*x[4] x[2]*x[4] x[4]^2 - x[1]*x[2] x[4]^4 x[4]^2 - x[1]*x[3];\nx[1]*x[5] x[2]*x[5] x[5]^2 - x[3]*x[5] x[4]^2 - x[1]*x[3] x[5]^4]\nG = Vector{Matrix{Polynomial{true, Int}}}(undef, 2)\nG[1] = [1 - x[1]^2 - x[2]^2 x[2]*x[3]; x[2]*x[3] 1 - x[3]^2]\nG[2] = [1 - x[4]^2 x[4]*x[5]; x[4]*x[5] 1 - x[5]^2]\n@time opt,data = tssos_first(F, G, x, 3, TS=\"MD\") # compute the first TS step of the TSSOS hierarchy\n@time opt,data = tssos_higher!(data, TS=\"MD\") # compute higher TS steps of the TSSOS hierarchy","category":"page"},{"location":"pmo/#Keyword-arguments","page":"Polynomial Matrix Optimization","title":"Keyword arguments","text":"","category":"section"},{"location":"pmo/","page":"Polynomial Matrix Optimization","title":"Polynomial Matrix Optimization","text":"Argument Description Default value\nCS Types of chordal extensions in exploiting correlative sparsity: \"MF\" (approximately smallest chordal extension), \"NC\" (not performing chordal extension), false (invalidating correlative sparsity exploitation) \"MF\"\ncliques Use customized variable cliques []\nTS Types of chordal extensions used in term sparsity iterations: \"block\"(maximal chordal extension), \"MD\" (approximately smallest chordal extension), false (invalidating term sparsity iterations) \"block\"\nQUIET Silence the output false\nsolve Solve the SDP relaxation true\nGram Output Gram matrices false","category":"page"},{"location":"pmo/#References","page":"Polynomial Matrix Optimization","title":"References","text":"","category":"section"},{"location":"pmo/","page":"Polynomial Matrix Optimization","title":"Polynomial Matrix Optimization","text":"Sparse Polynomial Matrix Optimization, Jared Miller, Jie Wang, and Feng Guo, 2024.","category":"page"},{"location":"sorf/#Sum-Of-Rational-Functions-Optimization","page":"Sum-Of-Rational-Functions Optimization","title":"Sum-Of-Rational-Functions Optimization","text":"","category":"section"},{"location":"sorf/","page":"Sum-Of-Rational-Functions Optimization","title":"Sum-Of-Rational-Functions Optimization","text":"The sum-of-rational-functions optimization problem could be formulized as","category":"page"},{"location":"sorf/","page":"Sum-Of-Rational-Functions Optimization","title":"Sum-Of-Rational-Functions Optimization","text":"mathrminf_mathbfxinmathbfK sum_i=1^Nfracp_i(mathbfx)q_i(mathbfx)","category":"page"},{"location":"sorf/","page":"Sum-Of-Rational-Functions Optimization","title":"Sum-Of-Rational-Functions Optimization","text":"where p_iq_iinmathbbRmathbfx are polynomials and mathbfK is the basic semialgebraic set","category":"page"},{"location":"sorf/","page":"Sum-Of-Rational-Functions Optimization","title":"Sum-Of-Rational-Functions Optimization","text":"mathbfKcoloneqqlbrace mathbfxinmathbbR^n mid g_i(mathbfx)ge0 i=1ldotsm h_j(mathbfx)=0 j=1ldotsellrbrace","category":"page"},{"location":"sorf/","page":"Sum-Of-Rational-Functions Optimization","title":"Sum-Of-Rational-Functions Optimization","text":"for some polynomials g_ih_jinmathbbRmathbfx.","category":"page"},{"location":"sorf/","page":"Sum-Of-Rational-Functions Optimization","title":"Sum-Of-Rational-Functions Optimization","text":"The following is a simple example.","category":"page"},{"location":"sorf/","page":"Sum-Of-Rational-Functions Optimization","title":"Sum-Of-Rational-Functions Optimization","text":"@polyvar x y z\np = [x^2 + y^2 - y*z, y^2 + x^2*z, z^2 - x + y] # define the vector of denominators\nq = [1 + 2x^2 + y^2 + z^2, 1 + x^2 + 2y^2 + z^2, 1 + x^2 + y^2 + 2z^2] # define the vector of numerator\ng = [1 - x^2 - y^2 - z^2]\nd = 2 # set the relaxation order\nopt = SumOfRatios(p, q, g, [], [x;y;z], d, QUIET=true, SignSymmetry=true) # Without correlative sparsity\nopt = SparseSumOfRatios(p, q, g, [], [x;y;z], d, QUIET=true, SignSymmetry=true) # With correlative sparsity","category":"page"},{"location":"sorf/#Keyword-arguments","page":"Sum-Of-Rational-Functions Optimization","title":"Keyword arguments","text":"","category":"section"},{"location":"sorf/","page":"Sum-Of-Rational-Functions Optimization","title":"Sum-Of-Rational-Functions Optimization","text":"Argument Description Default value\nSignSymmetry Exploit sign symmetries true\nGroebnerbasis Work in the quotient ring by computing a GrÃ¶bner basis false","category":"page"},{"location":"sorf/#Methods","page":"Sum-Of-Rational-Functions Optimization","title":"Methods","text":"","category":"section"},{"location":"sorf/","page":"Sum-Of-Rational-Functions Optimization","title":"Sum-Of-Rational-Functions Optimization","text":"SumOfRatios\nSparseSumOfRatios","category":"page"},{"location":"sorf/#TSSOS.SumOfRatios","page":"Sum-Of-Rational-Functions Optimization","title":"TSSOS.SumOfRatios","text":"optimum = SumOfRatios(p, q, g, h, x, d; QUIET=false, SignSymmetry=true, Groebnerbasis=false)\n\nMinimizing the sum of ratios p1/q1 + ... + pN/qN on the set defined by g >= 0 and h == 0.\n\nInput arguments\n\np: vector of denominators\nq: vector of numerator\ng: inequality constraints\nh: equality constraints\nx: vector of variables\nd: relaxation order\n\nOutput arguments\n\nSignSymmetry: exploit sign symmetries or not (true, false)\nGroebnerbasis: exploit the quotient ring structure or not (true, false)\n\n\n\n\n\n","category":"function"},{"location":"sorf/#TSSOS.SparseSumOfRatios","page":"Sum-Of-Rational-Functions Optimization","title":"TSSOS.SparseSumOfRatios","text":"optimum = SparseSumOfRatios(p, q, g, h, x, d; QUIET=false, SignSymmetry=true, Groebnerbasis=false)\n\nMinimizing the sum of sparse ratios p1/q1 + ... + pN/qN on the set defined by g >= 0 and h == 0.\n\nInput arguments\n\np: vector of denominators\nq: vector of numerator\ng: inequality constraints\nh: equality constraints\nx: vector of variables\nd: relaxation order\n\nOutput arguments\n\nSignSymmetry: exploit sign symmetries or not (true, false)\nGroebnerbasis: exploit the quotient ring structure or not (true, false)\n\n\n\n\n\n","category":"function"},{"location":"sorf/#References","page":"Sum-Of-Rational-Functions Optimization","title":"References","text":"","category":"section"},{"location":"sorf/","page":"Sum-Of-Rational-Functions Optimization","title":"Sum-Of-Rational-Functions Optimization","text":"Exploiting Sign Symmetries in Minimizing Sums of Rational Functions, Feng Guo, Jie Wang, and Jianhao Zheng, 2024.","category":"page"},{"location":"opf/#AC-Optimal-Power-Flow","page":"AC Optimal Power Flow","title":"AC Optimal Power Flow","text":"","category":"section"},{"location":"sos/#Sum-Of-Squares-Optimization","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"","category":"section"},{"location":"sos/","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"A general sum-of-squares optimization (including polynomial optimization as a special case) problem takes the form:","category":"page"},{"location":"sos/","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"mathrminf_mathbfyinmathbbR^n mathbfc^intercalmathbfy","category":"page"},{"location":"sos/","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"mathrmst a_k0+y_1a_k1+cdots+y_na_kninmathrmSOS k=1ldotsm","category":"page"},{"location":"sos/","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"where mathbfcinmathbbR^n and a_kiinmathbbRmathbfx are polynomials. In TSSOS, SOS constraints could be handled with the routine add_psatz!:","category":"page"},{"location":"sos/","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"info = add_psatz!(model, nonneg, vars, ineq_cons, eq_cons, order, TS=\"block\", SO=1, Groebnerbasis=false)","category":"page"},{"location":"sos/","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"where nonneg is a nonnegative polynomial constrained to admit a Putinar's style SOS representation on the semialgebraic set defined by ineq_cons and eq_cons, and SO is the sparse order.","category":"page"},{"location":"sos/","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"The following is a simple exmaple.","category":"page"},{"location":"sos/","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"mathrmsup lambda","category":"page"},{"location":"sos/","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"mathrmst x_1^2 + x_1x_2 + x_2^2 + x_2x_3 + x_3^2 - lambda(x_1^2+x_2^2+x_3^2)=sigma+tau_1(x_1^2+x_2^2+y_1^2-1)+tau_2(x_2^2+x_3^2+y_2^2-1)","category":"page"},{"location":"sos/","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"sigmainmathrmSOSdeg(sigma)le2d tau_1tau_2inmathbbRmathbfxdeg(tau_1)deg(tau_2)le2d-2","category":"page"},{"location":"sos/","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"using JuMP\nusing MosekTools\nusing DynamicPolynomials\nusing MultivariatePolynomials\nusing TSSOS\n\n@polyvar x[1:3]\nf = x[1]^2 + x[1]*x[2] + x[2]^2 + x[2]*x[3] + x[3]^2\nd = 2 # set the relaxation order\n@polyvar y[1:2]\nh = [x[1]^2+x[2]^2+y[1]^2-1, x[2]^2+x[3]^2+y[2]^2-1]\nmodel = Model(optimizer_with_attributes(Mosek.Optimizer))\n@variable(model, lower)\nnonneg = f - lower*sum(x.^2)\ninfo = add_psatz!(model, nonneg, [x; y], [], h, d, TS=\"block\", Groebnerbasis=true)\n@objective(model, Max, lower)\noptimize!(model)","category":"page"},{"location":"sos/#Keyword-arguments","page":"Sum-Of-Squares Optimization","title":"Keyword arguments","text":"","category":"section"},{"location":"sos/","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"Argument Description Default value\nCS Types of chordal extensions in exploiting correlative sparsity: \"MF\" (approximately smallest chordal extension), \"NC\" (not performing chordal extension), false (invalidating correlative sparsity exploitation) \"MF\"\ncliques Use customized variable cliques []\nTS Types of chordal extensions used in term sparsity iterations: \"block\"(maximal chordal extension), \"signsymmetry\" (sign symmetries), \"MD\" (approximately smallest chordal extension), false (invalidating term sparsity iterations) \"block\"\nQUIET Silence the output false\nSO Specify the sparse order 1\nGroebnerbasis Work in the quotient ring by computing a GrÃ¶bner basis false","category":"page"},{"location":"sos/#Image-of-a-semialgebraic-set-by-a-polynomial-map","page":"Sum-Of-Squares Optimization","title":"Image of a semialgebraic set by a polynomial map","text":"","category":"section"},{"location":"sos/","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"Example 1, Section 6.1 from arXiv:1507.06143.  The goal is to approximate the image of the two-dimensional unit ball mathbfS = { mathbfx in mathbbR^2  x_1^2 + x_2^2 leq 1Â } under the polynomial application f(mathbfx)=(x_1+x_1 x_2 x_2-x_1^3)2.  We choose mathbfB=mathbfS since it can be checked that f(mathbfS) subset mathbfB. ","category":"page"},{"location":"sos/","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"using JuMP\nusing MosekTools\nusing DynamicPolynomials\nusing MultivariatePolynomials\nusing SpecialFunctions\nusing TSSOS\nusing Plots\n\n# Moments of the Lebesgue measure on the unit ball\nfunction momball(a)\n  n,m = size(a)\n  y = zeros(m)\n  for k = 1:m\n    if all(.!Bool.(rem.(a[:,k],2)))\n      y[k]=prod(gamma.((a[:,k].+1)/2))/gamma(1+(n+sum(a[:,k]))/2)\n    end\n  end\n  return y\nend\n\n# Half-degree of polynomials w(x) and v(x)\nd = 5\n\nn = 2\n@polyvar x[1:n] \nf = [x[1] + x[1]*x[2]; x[2] - x[1]^3] * 0.5\ngS = 1 - x[1]^2 - x[2]^2\ngB = gS\nmodel = Model(optimizer_with_attributes(Mosek.Optimizer))\n#set_optimizer_attribute(model, MOI.Silent(), true)\n\nv, vc, vb = add_poly!(model, x, 2d)\nw, wc, wb = add_poly!(model, x, 2d)\nvf = subs(v, x[1]=>f[1], x[2]=>f[2])\ndv = Int(ceil(maxdegree(vf)/2))\n\n# Constraints\ninfo1 = add_psatz!(model, vf, x, [gS], [], dv, QUIET=false, CS=false, TS=false, Groebnerbasis=false) # v o f >= 0 on S\ninfo2 = add_psatz!(model, w-1-v, x, [gB], [], d, QUIET=false, CS=false, TS=false, Groebnerbasis=false) # w >= v + 1 on B\ninfo3 = add_psatz!(model, w, x, [gB], [], d, QUIET=false, CS=false, TS=false, Groebnerbasis=false) # w >= 0 on B\n\nsupp = TSSOS.get_basis(n, 2d)\n\n# Lebesgue moments on B\n\nmoment = momball(supp)\n@objective(model, Min, moment'*wc) # minimization of int w d_lambda\n\n\noptimize!(model)\nstatus = termination_status(model)\nif status != MOI.OPTIMAL\n    println(\"termination status: $status\")\n    status = primal_status(model)\n    println(\"solution status: $status\")\nend\nobjv = objective_value(model)\nwp = value.(wc)'*wb\n\n# Plot the superlevel set of w-1\nx1 = range(-1, 1, length=1000)\nx2 = range(-1, 1, length=1000)\nhw(x1, x2) = if x1^2 + x2^2 <= 1.0 wp(x1,x2) else 0.0 end\nzw = @. hw(x1', x2)\np = contour(x1, x2, zw, level=[1], color=[:white,:gray], levels=1, cbar=false, grid=false, fill=true)\n\n# Sample the image set f(S)\nN = 10^5\nX = randn(2, N)\nX = mapslices(c -> rand(1)[1]*c/sqrt(sum(c.^2)), X, dims=1)\nf1 = mapslices(c->f[1](c), X, dims=1)[1, :]\nf2 = mapslices(c->f[2](c), X, dims=1)[1, :]\nscatter!(p, f1, f2, mc=:black, legend=false)\n\n# Draw the unit circle\nt = range(0, 2*pi, length=100)\nxt = cos.(t)\nyt = sin.(t)\nplot!(p, xt, yt, color=:black, legend=false, ylimits=(-1,1), xlimits=(-1,1), aspect_ratio=:equal)","category":"page"},{"location":"sos/","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"<div style=\"align: center; text-align:center;\">     <img src=\"https://homepages.laas.fr/vmagron/files/tssos/image5.png\" alt=\"Image approximation\" style=\"width:40%; border:0;\" /> </div>","category":"page"},{"location":"sos/","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"The black dots correspond to the image set of the points obtained by uniform sampling of mathbfS under f.  The outer approximation obtained at the 5-th relaxation order is represented in light gray.","category":"page"},{"location":"sos/#Region-of-attraction-of-the-Van-der-Pol-oscillator","page":"Sum-Of-Squares Optimization","title":"Region of attraction of the Van der Pol oscillator","text":"","category":"section"},{"location":"sos/","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"Section 9.2 from arXiv:1208.1751.  The goal is to approximate the region of attraction of the uncontrolled reversed-time Van der Pol oscillator given by ","category":"page"},{"location":"sos/","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"dotx_1 = -2 x_2","category":"page"},{"location":"sos/","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"dotx_2 = 08 x_1 + 10 (x_1^2-021)x_2","category":"page"},{"location":"sos/","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"with general state constraints mathbfX=-12 12^2, terminal state constraints mathbfX_T = { mathbfx in mathbbR^2  x_1^2 + x_2^2 leq 001^2Â }, and final time T=100.","category":"page"},{"location":"sos/","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"using JuMP\nusing MosekTools\nusing DynamicPolynomials\nusing MultivariatePolynomials\nusing TSSOS\nusing Plots\n\nn = 2\n@polyvar x[1:n] t\n\n# Half-degree of polynomials w(x) and v(t,x)\nd = 8\n\n# Constraint set X = {x : ||x||_inf <= xb}\nxb = 1.2\ngx1 = xb^2 - x[1]^2\ngx2 = xb^2 - x[2]^2\ngX = [gx1; gx2]\n\n# Final time\nT = 100\n\n# Dynamics (scaled by final time)\nf = -[2*x[2], -0.8*x[1] - 10*(x[1]^2 - 0.20)*x[2]] * T\n\n# X_T\ngxT = (0.1^2 - x'*x)\n\nmodel = Model(optimizer_with_attributes(Mosek.Optimizer))\nset_optimizer_attribute(model, MOI.Silent(), true)\n\n# Define polynomials w(x) and v(t,x)\nv, vc, vb = add_poly!(model, [x;t], 2d)\nw, wc, wb = add_poly!(model, x, 2d)\nLv = sum([f;1] .* differentiate(v, [x;t]))\ndv = Int(ceil(maxdegree(Lv)/2))\n\n# Constraints (Note that the dynamics was scaled by T, so there T = 1)\n\ninfo1 = add_psatz!(model, -Lv, [x;t], [gX; t*(1-t)], [], dv, QUIET=false, CS=false, TS=false, Groebnerbasis=false) # Lv <= 0 on [0 T] x X\ninfo2 = add_psatz!(model, subs(v,t=>1), x, [gxT], [], d, QUIET=false, CS=false, TS=false, Groebnerbasis=false) # v >= 0 on {T} x X_T\ninfo3 = add_psatz!(model, w-1-subs(v,t=>0), x, gX, [], d, QUIET=false, CS=false, TS=false, Groebnerbasis=false) # w >= v + 1 on {0} x X\ninfo4 = add_psatz!(model, w, x, gX, [], d, QUIET=false, CS=false, TS=false, Groebnerbasis=false) # w >= 0 on X\n\nsupp = TSSOS.get_basis(n, 2d)\n\n# Lebesgue moments on X\n\nmoment = get_moment(n, supp, -xb*ones(n), xb*ones(n))\n@objective(model, Min, moment'*wc) # minimization of int w d_lambda\n\noptimize!(model)\nstatus = termination_status(model)\nif status != MOI.OPTIMAL\n    println(\"termination status: $status\")\n    status = primal_status(model)\n    println(\"solution status: $status\")\nend\nobjv = objective_value(model)\n\n\n# Plots\n\n# Plot the superlevel set of v\nvp = subs(value.(vc)'*vb, t=>0)\nwp = value.(wc)'*wb\n\nx1 = range(-xb, xb, length=1000)\nx2 = range(-xb, xb, length=1000)\nhw(x1,x2) = wp(x1, x2)\nhv(x1,x2) = max(vp(x1,x2), -0.1)\nzw = @. hw(x1', x2)\nzv = @. hv(x1', x2)\n\np = contour(x1, x2, zv, level=[0], color=[:white,:gray], levels=1, cbar=false,grid=false,fill=true, ylimits=(-xb,xb), xlimits=(-xb,xb), aspect_ratio=:equal)\n\n# Simulate trajectory with reversed time to get the boundary of the true ROA\nusing DifferentialEquations\n\nroafun(X, p, t) = [2*X[2]; -0.8*X[1] - 10*(X[1]^2-0.21)*X[2]]\nprob = ODEProblem(roafun, [0.1;0.1], (0.0,100.0))\nsolode = solve(prob,DP5(), reltol=1e-8, abstol=1e-8)\nxt = map(v -> v[1], solode.u)\nyt = map(v -> v[2], solode.u)\n\nplot!(p, xt[1500:end], yt[1500:end], color=:black, legend=false, ylimits=(-xb,xb), xlimits=(-xb,xb), aspect_ratio=:equal)","category":"page"},{"location":"sos/","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"<div style=\"align: center; text-align:center;\">     <img src=\"https://homepages.laas.fr/vmagron/files/tssos/roa8.png\" alt=\"Image approximation\" style=\"width:40%; border:0;\" /> </div>","category":"page"},{"location":"sos/","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"The black curve corresponds to the boundary of the true region of attraction.  The outer approximation obtained at the 8-th relaxation order is represented in light gray.","category":"page"},{"location":"sos/#Methods","page":"Sum-Of-Squares Optimization","title":"Methods","text":"","category":"section"},{"location":"sos/","page":"Sum-Of-Squares Optimization","title":"Sum-Of-Squares Optimization","text":"add_psatz!\nadd_poly!\nadd_SOS!","category":"page"},{"location":"sos/#TSSOS.add_psatz!","page":"Sum-Of-Squares Optimization","title":"TSSOS.add_psatz!","text":"info = add_psatz!(model, nonneg, vars, ineq_cons, eq_cons, order; CS=false, cliques=[], TS=\"block\", \nSO=1, Groebnerbasis=false, QUIET=false, constrs=nothing)\n\nAdd a Putinar's style SOS representation of the polynomial nonneg to the JuMP model.\n\nInput arguments\n\nmodel: a JuMP optimization model\nnonneg: a nonnegative polynomial constrained to be a Putinar's style SOS on a semialgebraic set\nvars: the set of POP variables\nineq_cons: inequality constraints\neq_cons: equality constraints\norder: relaxation order\nCS: method of chordal extension for correlative sparsity (\"MF\", \"MD\", \"NC\", false)\ncliques: the set of cliques used in correlative sparsity\nTS: type of term sparsity (\"block\", \"signsymmetry\", \"MD\", \"MF\", false)\nSO: sparse order\nGroebnerbasis: exploit the quotient ring structure or not (true, false)\nQUIET: run in the quiet mode (true, false)\nconstrs: the constraint name used in the JuMP model\n\nOutput arguments\n\ninfo: auxiliary data\n\n\n\n\n\n","category":"function"},{"location":"sos/#TSSOS.add_poly!","page":"Sum-Of-Squares Optimization","title":"TSSOS.add_poly!","text":"p,coe,mon = add_poly!(model, vars, degree)\n\nGenerate an unknown polynomial of given degree whose coefficients are from the JuMP model.\n\nInput arguments\n\nmodel: a JuMP optimization model\nvars: set of variables\ndegree: degree of the polynomial\n\nOutput arguments\n\np: the polynomial \ncoe: coefficients of the polynomial \nmon: monomials of the polynomial \n\n\n\n\n\n","category":"function"},{"location":"sos/#TSSOS.add_SOS!","page":"Sum-Of-Squares Optimization","title":"TSSOS.add_SOS!","text":"sos = add_SOS!(model, vars, d)\n\nGenerate an SOS polynomial of degree 2d whose coefficients are from the JuMP model.\n\nInput arguments\n\nmodel: a JuMP optimization model\nvars: set of variables\nd: half degree of the SOS polynomial\n\nOutput arguments\n\nsos: the sos polynomial \n\n\n\n\n\nsos = add_SOS!(model, basis)\n\nGenerate an SOS polynomial with monomial basis whose coefficients are from the JuMP model.\n\nInput arguments\n\nmodel: a JuMP optimization model\nbasis: monomial basis\n\nOutput arguments\n\nsos: the sos polynomial \n\n\n\n\n\n","category":"function"},{"location":"technique/#Techniques","page":"Techniques","title":"Techniques","text":"","category":"section"},{"location":"technique/#Homogenization","page":"Techniques","title":"Homogenization","text":"","category":"section"},{"location":"technique/#Christoffel-Darboux-Kernels","page":"Techniques","title":"Christoffel-Darboux Kernels","text":"","category":"section"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"Here is the list of functions implemented in TSSOS that are based on computing Christiffel-Darboux kernels associated to a given POP:","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"run_H1\nrun_H1CS\nrun_H2\nrun_H2CS\nconstruct_CDK\nconstruct_marginal_CDK\nconstruct_CDK_cs\nconstruct_marginal_CDK_cs","category":"page"},{"location":"technique/#TSSOS.run_H1","page":"Techniques","title":"TSSOS.run_H1","text":"Performs iterative bound strengthening (i.e, implements H1) for a given POP.\n\nArguments\n\nx::Vector: Initial vector of decision variables.\nn::Int: Dimension or size of the problem.\nd::Int: Degree of the relaxation.\ndc::Int: Degree of the Christoffel polynomials relaxation. dc must be smaller than or equal to d.\nN::Int: Maximum number of iterations to perform.\neps::Float64: Perturbation factor deciding the volume of CDK sublevel sets.\ngap_tol::Float64 (optional): Threshold for optimality gap tolerance. Default is 0.5.\nbeta::Float64 (optional): Threshold beta controlling the strength of the Tikhonov regularization of CDK. Default is 0.001.\n\nReturns\n\nA list with the following elements:\n\nOPT: Relaxation bound from each iteration.\ngap_history: A history of the optimality gaps at each iteration.\nMoment_matrices: Moment matrices computed during each iterations used to construct CDK sublevel constraints.\nK: Parameter to keep track of the rank of the moment matrices for each iteration.\nCertain_overrestriction: A measure or flag for over-restriction (when H1 is sure that further feasible set reductions would yield an invalid bound).\nubRef: Upper bounds with respect to which the optimality gap is measured.\n\n\n\n\n\n","category":"function"},{"location":"technique/#TSSOS.run_H2","page":"Techniques","title":"TSSOS.run_H2","text":"Applies H2 algorithm to a given POP instance\n\nArguments\n\npop::Vector: Vector defining the problem, the first element being the objective function, the others being constraints.\nx::Vector: Initial vector of decision variables \nd::Int: - Degree of the relaxation\ndc::Int: - Degree of the marginal Christoffel polynomials (has to be smaller than or equal to d - order of the moment matrix)\nlocal_sol::Vector: Available solution (here obtained from an external local optimizer IPOPT).\ntau::Float64: Filtering parameter \nbeta::Float64 (optional): Threshold beta controling the strength of the Tikhonov regularization of marginal Christoffel polynomials. Default is 0.001.\n\nReturns\n\nA tuple of the following:\n\n[opt, optcdk]: A pair containing the optimal solutions of moment relaxation before and after CDK strengthening via H2.\n[initial_gap, gapcdk]: Relative optimality gaps before and after CDK strengthening via H2.\nGammas: Thresholds used to construct sublevel sets of marginal Christoffel polynomials (CDK).\n[ubRef, f(solcdk)]: The available upper bound before and after CDK strengthening via H2.\ndata.moment[1]: The moment matrix from which marginal CDK polynomials were constructed    \n\n\n\n\n\n","category":"function"},{"location":"technique/#TSSOS.run_H2CS","page":"Techniques","title":"TSSOS.run_H2CS","text":"Applies H2CS algorithm to a given POP instance\n\nArguments\n\npop::Vector: Vector defining the problem, the first element being the objective function, the others being constraints.\nx::Vector: Initial vector of decision variables \nd::Int: - Degree of the relaxation\ndc::Int: - Degree of the marginal Christoffel polynomials (has to be smaller than or equal to d - order of the moment matrix)\nlocal_sol::Vector: Available solution (here obtained from an external local optimizer IPOPT).\ntau::Float64: Filtering parameter \nbeta::Float64 (optional): Threshold beta controling the strength of the Tikhonov regularization of marginal Christoffel polynomials. Default is 0.001.\n\nReturns\n\nA tuple of the following:\n\n[opt, optcdk]: A pair containing the optimal solutions of the sparse moment relaxation before and after CDK strengthening via H2CS.\n[initial_gap, gapcdk]: Relative optimality gaps before and after CDK strengthening via H2CS.\nGammas: Thresholds used to construct sublevel sets of marginal Christoffel polynomials (CDK).\n[ubRef, f(solcdk)]: The available upper bound before and after CDK strengthening via H2CS.\ndata.moment[1]: The moment matrix from which marginal CDK polynomials were constructed.\n\n\n\n\n\n","category":"function"},{"location":"technique/#TSSOS.construct_CDK","page":"Techniques","title":"TSSOS.construct_CDK","text":"Constructs Christoffel polynomial of order d (CDK) using Singular Value Decomposition (SVD) for the input moment matrix `Mm` of order smaller or equal than d.\n\nArguments\n\nvars: vector of variables, e.g., @polyvar x[1:5]\ndc:  degree of the CDK (has to be smaller than or equal to the order of the moment matrix)\nMm::Matrix: The input moment matrix from which which the CDK is constructed.\nthreshold::Float64 (optional): A threshold value for filtering eigenvalues (i.e., deciding the numerical rank of Mm) . Default is 0.001.\n\nReturns\n\np_alpha_squared[1:negativeEV]: Polynomials in the kernel of the moment matrix.\ncdk: The constructed CDK (positive part of the moment matrix).\npositiveEV: The number of positive eigenvalues of the moment matrix.\nminimum(Qval): The minimum eigenvalue of the moment matrix.\nnegativeEV: The number of negative eigenvalues of the moment matrix.    \n\n\n\n\n\n","category":"function"},{"location":"technique/#TSSOS.construct_marginal_CDK","page":"Techniques","title":"TSSOS.construct_marginal_CDK","text":"Constructs marginal Christoffel polynomials (CDK) using Singular Value Decomposition (SVD) for the input matrix `Mm`.\n\nArguments\n\nvars: vector of variables, e.g., @polyvar x[1:5].\nk::vector - Index of the decision variable for which CDK needs to be constructed.\ndc::Int: - Degree of the marginal Christoffel polynomials (has to be smaller than or equal to d - order of the moment matrix).\nMm::Matrix: The input moment matrix from which the marginal CDK is constructed.\nthreshold::Float64 (optional): A threshold value for filtering eigenvalue (i.e., deciding the numerical rank of Mm) . Default is 0.001.\n\nReturns\n\ncdk: The constructed marginal CDK.\np_alpha_squared[1:negativeEV]: Polynomials in the kernel of the marginal moment matrix.\npositiveEV: The set of positive eigenvalues of the marginal moment matrix.\nnegativeEV: The set of negative eigenvalues of the marginal moment matrix.\nminimum(Qval): The minimum eigenvalue of the marginal moment matrix.    \n\n\n\n\n\n","category":"function"},{"location":"technique/#TSSOS.construct_CDK_cs","page":"Techniques","title":"TSSOS.construct_CDK_cs","text":"construct_CDK_cs(x, dc, Mm, cliques, threshold=0.001) -> Tuple\nConstruct the `Cdk`, kernel, and eigenvalue-related statistics for a given set of cliques and moment matrices.\n\nArguments\n\nx::Vector: A vector of polynomial variables.\ndc::Int: The degree of the Christoffel polynomial constraints. It must be  <= than the order of relaxation d.\nMm::Vector: A vector of moment matrices, one for each clique.\ncliques::Vector{Vector{Int}}: A list of cliques, where each clique is a vector of indices corresponding to variables in x.\nthreshold::Float64 (optional): A small positive value used to distinguish between eigenvalues considered positive and negative. Defaults to 0.001.\n\nReturns\n\nTuple: A tuple containing:\nKernel::Vector: A vector of squared orthonormal polynomial values corresponding to the polynomials in the kernel of clique-based moment matrices.\nCdk::Vector: A vector of Christoffel poolynomials of degree 2*dc, one for each clique.\nposEV::Vector: A vector of counts of eigenvalues above the threshold for each clique.\nminEV::Vector: A vector of the smallest eigenvalue for each clique.\nnegEV::Vector: A vector of counts of eigenvalues below or equal to the threshold for each clique. \n\n\n\n\n\n","category":"function"},{"location":"technique/#TSSOS.construct_marginal_CDK_cs","page":"Techniques","title":"TSSOS.construct_marginal_CDK_cs","text":"construct_marginal_CDK_cs(vars, k, dc, Mm, cliques; threshold=0.001) -> Tuple\n\nConstructs the marginal Christoffel polynomials (CDK) for a given variable x_k using eigenvalue decomposition, using clique-based moment matrices.\n\nArguments\n\nvars::Vector: A vector of polynomial variables (e.g., created with @polyvar x[1:n]).\nk::Int: The global index of the variable x_k  for which the marginal CDK is to be constructed.\ndc::Int: The degree of the marginal Christoffel polynomials. Must be less than or equal to the order of the associated moment matrix.\nMm::Vector{Matrix}: A vector of moment matrices, where each matrix corresponds to a clique.\ncliques::Vector{Vector{Int}}: A list of cliques, where each clique is a subset of the indices of vars.\nthreshold::Float64 (optional): A threshold for filtering eigenvalues, used to determine the numerical rank of the moment matrix. Default is 0.001.\n\nReturns\n\nA tuple containing:\n\np_alpha_squared[1:negativeEV]::Vector: Polynomials in the kernel of the marginal moment matrix.\ncdk::Float64: The constructed marginal Christoffel polynomial.\npositiveEV::Int: The count of positive eigenvalues of the marginal moment matrix.\nnegativeEV::Int: The count of negative eigenvalues of the marginal moment matrix.\nminimum(Qval)::Float64: The smallest eigenvalue of the marginal moment matrix.\n\n\n\n\n\n","category":"function"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"We provide below some illustrations.  Let us consider the following POP (Example 3.2 from [Sparse polynomial optimization: theory and practice]):","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"n = 6\n@polyvar x[1:n]\nf = x[2]*x[5] + x[3]*x[6] - x[2]*x[3] - x[5]*x[6] + x[1]*(-x[1] + x[2] + x[3] - x[4] + x[5] + x[6])\ng = [(6.36 - x[i]) * (x[i] - 4) for i in 1:6]\npop = [f, g...]\nd = 1","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"We can solve the problem using the dense moment-SOS hierarchy:","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"opt, sol, data = cs_tssos_first(pop, x, d, TS=false, CS=false, solution=true)","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"Afterwards, one can try strenghtening the bound via H1:","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"N = 5\neps = 0.05\ndc = 1\ngap_tol = 0.1\nresultH1 = run_H1(pop,x,d,dc,N,eps,gap_tol)","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"or via H2","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"dc = 1\nlocal_sol = sol\ntau = 1.1\nresultH2 = run_H2(pop,x,d,dc,local_sol,tau)","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"Alternatively, the problem can be solved using the correlatively sparse moment-SOS hierarchy:","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"opt, sol, data = cs_tssos_first(pop, x, d, TS=false, solution=true)","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"Afterwards, the bound can be strengthened either via H1CS:","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"N = 5\neps = 0.05\ndc = 1\ngap_tol = 0.1\nresultH1CS = run_H1CS(pop,x,d,dc,N,eps,gap_tol)","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"or via H2CS","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"dc = 1\nlocal_sol = sol\ntau = 1.1\nresultH2CS = run_H2CS(pop,x,d,dc,local_sol,tau)","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"Moreover, here is how different Christoffel polynomials can be constructed using the output of the: ","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"dense Moment-SOS relaxation of order 2","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"d = 2\nopt, sol, data = cs_tssos_first(pop, x, d, TS=false, CS=false, solution=true, Mommat=true)\n\nk = 4\ndc = 1\nCDK_order1 = construct_CDK(x, dc, data.moment[1])  # Constructs multivariate Christoffel polynomial of order dc=1 (quadratic CDK)\nCDK_4_order1 = construct_marginal_CDK(x, k, dc, data.moment[1])  # Constructs marginal Christoffel polynomial, associated to x_4, of order dc=1 \n\ndc = 2\nCDK_order2 = construct_CDK(x, dc, data.moment[1])  # Construct multivariate Christoffel polynomial of order dc=2 (quartic CDK)\nCDK_4_order2 = construct_marginal_CDK(x, k, dc, data.moment[1])  # Constructs marginal Christoffel polynomial, associated to x_4, of order dc=2 \n","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"sparse Moment-SOS relaxation of order 2","category":"page"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"d = 2\nopt, sol, data = cs_tssos_first(pop, x, d, TS=false, solution=true, Mommat=true)\n\nk = 4\ndc = 1\nCDK_sparse_order1 = construct_CDK_cs(x, dc, data.moment, data.cliques)  # Constructs multivariate Christoffel polynomial of order dc=1 for each identified clique.\nCDK_sparse_4_order1 = construct_marginal_CDK_cs(x, k, dc, data.moment, data.cliques)  # Constructs marginal Christoffel polynomial, associated to x_4, of order dc=1 \n\ndc = 2\nCDK_sparse_order2 = construct_CDK_cs(x, dc, data.moment, data.cliques)  # Constructs multivariate Christoffel polynomial of order dc=2 for each identified clique.\nCDK_sparse_4_order2 = construct_marginal_CDK_cs(x, k, dc, data.moment, data.cliques)  # Constructs marginal Christoffel polynomial, associated to x_4, of order dc=2\n","category":"page"},{"location":"technique/#Tips-for-modelling-polynomial-optimization-problems","page":"Techniques","title":"Tips for modelling polynomial optimization problems","text":"","category":"section"},{"location":"technique/","page":"Techniques","title":"Techniques","text":"When possible, explictly include a sphere/ball constraint (or multi-sphere/multi-ball constraints).\nWhen the feasible set is unbounded, try the homogenization technique introduced in Homogenization for polynomial optimization with unbounded sets.\nScale the coefficients of the polynomial optimization problem to -1 1.\nScale the variables so that they take values in -1 1 or 0 1.\nTry to include more (redundant) inequality constraints.","category":"page"},{"location":"structure/#Structures","page":"Structures","title":"Structures","text":"","category":"section"},{"location":"structure/#Quotient-ring","page":"Structures","title":"Quotient ring","text":"","category":"section"},{"location":"structure/#Correlative-sparsity","page":"Structures","title":"Correlative sparsity","text":"","category":"section"},{"location":"structure/#Term-Sparsity","page":"Structures","title":"Term Sparsity","text":"","category":"section"},{"location":"structure/#Symmetry","page":"Structures","title":"Symmetry","text":"","category":"section"},{"location":"structure/","page":"Structures","title":"Structures","text":"tssos_symmetry","category":"page"},{"location":"structure/#TSSOS.tssos_symmetry","page":"Structures","title":"TSSOS.tssos_symmetry","text":"opt,basis,Gram = tssos_symmetry(pop, x, d, group; numeq=0, QUIET=false)\n\nCompute the symmetry adapted moment-SOS relaxation for polynomial optimization problems.\n\nInput arguments\n\npop: polynomial optimization problem\nx: POP variables\nd: relaxation order\ngroup: permutation group acting on POP variables \nnumeq: number of equality constraints\nQUIET: run in the quiet mode (true, false)\n\nOutput arguments\n\nopt: optimum\nbasis: symmetry adapted basis\nGram: Gram matrix\n\n\n\n\n\n","category":"function"},{"location":"#TSSOS","page":"Home","title":"TSSOS","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"TSSOS aims to provide a user-friendly and efficient tool for solving optimization problems with polynomials, which is based on the structured moment-SOS hierarchy.","category":"page"},{"location":"#Authors","page":"Home","title":"Authors","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Jie Wang, Academy of Mathematics and Systems Science, Chinese Academy of Sciences.\nVictor Magron, Laboratoire d'Architecture et Analyse des SystÃ¨mes, CNRS.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"TSSOS could be installed by running","category":"page"},{"location":"","page":"Home","title":"Home","text":"pkg> add https://github.com/wangjie212/TSSOS","category":"page"},{"location":"#Related-packages","page":"Home","title":"Related packages","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"DynamicPolynomials: Polynomial definition\nMultivariatePolynomials: Polynomial manipulation\nNCTSSOS: Noncommutative polynomial optimization\nChordalGraph: Chordal graphs and chordal extentions\nSparseJSR: Computing joint spetral radius","category":"page"},{"location":"#References","page":"Home","title":"References","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"TSSOS: A Moment-SOS hierarchy that exploits term sparsity, Jie Wang, Victor Magron, and Jean B. Lasserre, 2021.\nChordal-TSSOS: a moment-SOS hierarchy that exploits term sparsity with chordal extension, Jie Wang, Victor Magron, and Jean B. Lasserre, 2021.\nCS-TSSOS: Correlative and term sparsity for large-scale polynomial optimization, Jie Wang, Victor Magron, Jean B. Lasserre, and Ngoc H. A. Mai, 2022.\nTSSOS: a Julia library to exploit sparsity for large-scale polynomial optimization, Victor Magron and Jie Wang, 2021.","category":"page"},{"location":"cpop/#Complex-Polynomial-Optimization","page":"Complex Polynomial Optimization","title":"Complex Polynomial Optimization","text":"","category":"section"},{"location":"cpop/","page":"Complex Polynomial Optimization","title":"Complex Polynomial Optimization","text":"A general complex polynomial optimization problem could be formulized as","category":"page"},{"location":"cpop/","page":"Complex Polynomial Optimization","title":"Complex Polynomial Optimization","text":"mathrminf_mathbfzinmathbfK f(mathbfzbarmathbfz)","category":"page"},{"location":"cpop/","page":"Complex Polynomial Optimization","title":"Complex Polynomial Optimization","text":"with","category":"page"},{"location":"cpop/","page":"Complex Polynomial Optimization","title":"Complex Polynomial Optimization","text":"mathbfKcoloneqqlbrace mathbfzinmathbbC^n mid g_i(mathbfzbarmathbfz)ge0 i=1ldotsm h_j(mathbfzbarmathbfz)=0 j=1ldotsellrbrace","category":"page"},{"location":"cpop/","page":"Complex Polynomial Optimization","title":"Complex Polynomial Optimization","text":"where barmathbfz stands for the conjugate of mathbfz=(z_1ldotsz_n), and f g_i i=1ldotsm h_j j=1ldotsell are real-valued complex polynomials satisfying barf=f and barg_i=g_i, barh_j=h_j.","category":"page"},{"location":"cpop/","page":"Complex Polynomial Optimization","title":"Complex Polynomial Optimization","text":"In TSSOS, we use x_i to represent the complex variable z_i and use x_n+i to represent its conjugate barz_i. Let us consider the following example:","category":"page"},{"location":"cpop/","page":"Complex Polynomial Optimization","title":"Complex Polynomial Optimization","text":"mathrminf 3-z_1^2-05mathbfiz_1barz_2^2+05mathbfiz_2^2barz_1","category":"page"},{"location":"cpop/","page":"Complex Polynomial Optimization","title":"Complex Polynomial Optimization","text":"mathrmst z_2+barz_2ge0z_1^2-025z_1^2-025barz_1^2=1z_1^2+z_2^2=3mathbfiz_2-mathbfibarz_2=0","category":"page"},{"location":"cpop/","page":"Complex Polynomial Optimization","title":"Complex Polynomial Optimization","text":"which is represented in TSSOS as","category":"page"},{"location":"cpop/","page":"Complex Polynomial Optimization","title":"Complex Polynomial Optimization","text":"mathrminf 3-x_1x_3-05mathbfix_1x_4^2+05mathbfix_2^2x_3","category":"page"},{"location":"cpop/","page":"Complex Polynomial Optimization","title":"Complex Polynomial Optimization","text":"mathrmst x_2+x_4ge0x_1x_3-025x_1^2-025x_3^2=1x_1x_3+x_2x_4=3mathbfix_2-mathbfix_4=0","category":"page"},{"location":"cpop/","page":"Complex Polynomial Optimization","title":"Complex Polynomial Optimization","text":"using DynamicPolynomials\nn = 2 # set the number of complex variables\n@polyvar x[1:2n]\nf = 3 - x[1]*x[3] - 0.5im*x[1]*x[4]^2 + 0.5im*x[2]^2*x[3]\ng1 = x[2] + x[4]\nh1 = x[1]*x[3] - 0.25*x[1]^2 - 0.25 x[3]^2 - 1\nh2 = x[1]*x[3] + x[2]*x[4] - 3\nh3 = im*x[2] - im*x[4]\npop = [f, g, h1, h2, h3]\norder = 2 # set the relaxation order\nopt,sol,data = cs_tssos_first(pop, x, n, order, numeq=3) # compute the first TS step of the CS-TSSOS hierarchy\nopt,sol,data = cs_tssos_higher!(data) # compute higher TS steps of the CS-TSSOS hierarchy","category":"page"},{"location":"cpop/#Keyword-arguments","page":"Complex Polynomial Optimization","title":"Keyword arguments","text":"","category":"section"},{"location":"cpop/","page":"Complex Polynomial Optimization","title":"Complex Polynomial Optimization","text":"Argument Description Default value\nnb Specify the first nb complex variables to be of unit norm 0\nnumeq Specify the last numeq constraints to be equality constraints 0\nCS Types of chordal extensions in exploiting correlative sparsity: \"MF\" (approximately smallest chordal extension), \"NC\" (not performing chordal extension), false (invalidating correlative sparsity exploitation) \"MF\"\ncliques Use customized variable cliques []\nTS Types of chordal extensions used in term sparsity iterations: \"block\"(maximal chordal extension), \"MD\" (approximately smallest chordal extension), false (invalidating term sparsity iterations) \"block\"\nipart Use complex moment matrices true\nbalanced Impose the balanced structure false\nnormality Impose normality condtions of order normality 1\nmerge Merge overlapping PSD blocks false\nmd Parameter for tunning the merging strength 3\nMomentOne add a first-order moment PSD constraint for each variable clique false\nsolver Specify an SDP solver: \"Mosek\" or \"COSMO\" \"Mosek\"\ncosmo_setting Parameters for the COSMO solver: cosmo_para(eps_abs, eps_rel, max_iter, time_limit) cosmo_para(1e-5, 1e-5, 1e4, 0)\nmosek_setting Parameters for the Mosek solver: mosek_para(tol_pfeas, tol_dfeas, tol_relgap, time_limit, num_threads) mosek_para(1e-8, 1e-8, 1e-8, -1, 0)\nQUIET Silence the output false\nsolve Solve the SDP relaxation true\ndualize Solve the dual SDP problem false\nGram Output Gram matrices false\nsolution Extract an approximately optimal solution false\ntol Tolerance for certifying global optimality 1e-4","category":"page"},{"location":"cpop/#References","page":"Complex Polynomial Optimization","title":"References","text":"","category":"section"},{"location":"cpop/","page":"Complex Polynomial Optimization","title":"Complex Polynomial Optimization","text":"Exploiting Sparsity in Complex Polynomial Optimization, Jie Wang and Victor Magron, 2021.","category":"page"},{"location":"pop/#Polynomial-Optimization","page":"Polynomial Optimization","title":"Polynomial Optimization","text":"","category":"section"},{"location":"pop/","page":"Polynomial Optimization","title":"Polynomial Optimization","text":"Polynomial optimization concerns minimizing a polynomial subject to a tuple of polynomial inequality constraints and equality constraints, which in general takes the form:","category":"page"},{"location":"pop/","page":"Polynomial Optimization","title":"Polynomial Optimization","text":"mathrminf_mathbfxinmathbbR^n f(mathbfx) text st  g_1(mathbfx)ge0ldotsg_m(mathbfx)ge0h_1(mathbfx)=0ldotsh_ell(mathbfx)=0","category":"page"},{"location":"pop/","page":"Polynomial Optimization","title":"Polynomial Optimization","text":"where fg_1ldotsg_mh_1ldotsh_ellinmathbbRmathbfx are polynomials in variables mathbfx.","category":"page"},{"location":"pop/","page":"Polynomial Optimization","title":"Polynomial Optimization","text":"To illustrate how to solve a polynomial optimization problem with TSSOS, let us consider f=1+x_1^4+x_2^4+x_3^4+x_1x_2x_3+x_2 and g=1-x_1^2-2x_2^2 h=x_2^2+x_3^2-1.","category":"page"},{"location":"pop/","page":"Polynomial Optimization","title":"Polynomial Optimization","text":"@polyvar x[1:3]\nf = 1 + x[1]^4 + x[2]^4 + x[3]^4 + x[1]*x[2]*x[3] + x[2]\ng = 1 - x[1]^2 - 2*x[2]^2\nh = x[2]^2 + x[3]^2 - 1\npop = [f, g, h]\nd = 2 # set the relaxation order\nopt,sol,data = tssos_first(pop, x, d, numeq=1, TS=\"MD\") # compute the first TS step of the TSSOS hierarchy\nopt,sol,data = tssos_higher!(data, TS=\"MD\") # compute higher TS steps of the TSSOS hierarchy","category":"page"},{"location":"pop/#Keyword-arguments","page":"Polynomial Optimization","title":"Keyword arguments","text":"","category":"section"},{"location":"pop/","page":"Polynomial Optimization","title":"Polynomial Optimization","text":"Argument Description Default value\nnb Specify the first nb variables to be pm1 binary variables 0\nnumeq Specify the last numeq constraints to be equality constraints 0\nGroebnerbasis Work in the quotient ring by computing a GrÃ¶bner basis true\nbasis Use customized monomial bases []\nreducebasis Reduce the monomial bases false\nTS Types of chordal extensions used in term sparsity iterations: \"block\"(maximal chordal extension), \"signsymmetry\" (sign symmetries), \"MD\" (approximately smallest chordal extension), false (invalidating term sparsity exploitation) \"block\"\nnormality Impose normality condtions false\nmerge Merge overlapping PSD blocks false\nmd Parameter for tunning the merging strength 3\nMomentOne add a first-order moment PSD constraint false\nsolver Specify an SDP solver: \"Mosek\" or \"COSMO\" \"Mosek\"\ncosmo_setting Parameters for the COSMO solver: cosmo_para(eps_abs, eps_rel, max_iter, time_limit) cosmo_para(1e-5, 1e-5, 1e4, 0)\nmosek_setting Parameters for the Mosek solver: mosek_para(tol_pfeas, tol_dfeas, tol_relgap, time_limit, num_threads) mosek_para(1e-8, 1e-8, 1e-8, -1, 0)\nQUIET Silence the output false\nsolve Solve the SDP relaxation true\ndualize Solve the dual SDP problem false\nGram Output Gram matrices false\nsolution Extract optimal solutions false\ntol Tolerance for certifying global optimality 1e-4","category":"page"},{"location":"pop/#Correlative-sparsity","page":"Polynomial Optimization","title":"Correlative sparsity","text":"","category":"section"},{"location":"pop/","page":"Polynomial Optimization","title":"Polynomial Optimization","text":"The following is an example where one exploits correlative sparsity and term sparsity simultaneously.","category":"page"},{"location":"pop/","page":"Polynomial Optimization","title":"Polynomial Optimization","text":"using DynamicPolynomials\nn = 6\n@polyvar x[1:n]\nf = 1 + sum(x.^4) + x[1]*x[2]*x[3] + x[3]*x[4]*x[5] + x[3]*x[4]*x[6]+x[3]*x[5]*x[6] + x[4]*x[5]*x[6]\npop = [f, 1-sum(x[1:3].^2), 1-sum(x[3:6].^2)]\norder = 2 # set the relaxation order\nopt,sol,data = cs_tssos_first(pop, x, order, numeq=0, TS=\"MD\") # compute the first TS step of the CS-TSSOS hierarchy\nopt,sol,data = cs_tssos_higher!(data, TS=\"MD\") # compute higher TS steps of the CS-TSSOS hierarchy","category":"page"},{"location":"pop/#Keyword-arguments-2","page":"Polynomial Optimization","title":"Keyword arguments","text":"","category":"section"},{"location":"pop/","page":"Polynomial Optimization","title":"Polynomial Optimization","text":"Argument Description Default value\nnb Specify the first nb variables to be pm1 binary variables 0\nnumeq Specify the last numeq constraints to be equality constraints 0\nCS Types of chordal extensions in exploiting correlative sparsity: \"MF\" (approximately smallest chordal extension), \"NC\" (not performing chordal extension), false (invalidating correlative sparsity exploitation) \"MF\"\nbasis Use customized monomial bases []\nhbasis Use customized monomial bases associated with equality constraints []\ncliques Use customized variable cliques []\nTS Types of chordal extensions used in term sparsity iterations: \"block\"(maximal chordal extension), \"signsymmetry\" (sign symmetries), \"MD\" (approximately smallest chordal extension), false (invalidating term sparsity iterations) \"block\"\nmerge Merge overlapping PSD blocks false\nmd Parameter for tunning the merging strength 3\nMomentOne add a first-order moment PSD constraint for each variable clique false\nsolver Specify an SDP solver: \"Mosek\" or \"COSMO\" \"Mosek\"\ncosmo_setting Parameters for the COSMO solver: cosmo_para(eps_abs, eps_rel, max_iter, time_limit) cosmo_para(1e-5, 1e-5, 1e4, 0)\nmosek_setting Parameters for the Mosek solver: mosek_para(tol_pfeas, tol_dfeas, tol_relgap, time_limit, num_threads) mosek_para(1e-8, 1e-8, 1e-8, -1, 0)\nQUIET Silence the output false\nsolve Solve the SDP relaxation true\ndualize Solve the dual SDP problem false\nGram Output Gram matrices false\nsolution Extract an approximately optimal solution false\ntol Tolerance for certifying global optimality 1e-4","category":"page"},{"location":"pop/#Compute-a-locally-optimal-solution","page":"Polynomial Optimization","title":"Compute a locally optimal solution","text":"","category":"section"},{"location":"pop/","page":"Polynomial Optimization","title":"Polynomial Optimization","text":"Moment-SOS relaxations provide lower bounds on the optimum of the polynomial optimization problem. As the complementary side, one could compute a locally optimal solution which provides an upper bound on the optimum of the polynomial optimization problem. The upper bound is useful in evaluating the quality (tightness) of those lower bounds provided by moment-SOS relaxations. In TSSOS, for a given polynomial optimization problem, a locally optimal solution could be obtained via the nonlinear programming solver Ipopt:","category":"page"},{"location":"pop/","page":"Polynomial Optimization","title":"Polynomial Optimization","text":"obj,sol,status = local_solution(data.n, data.m, data.supp, data.coe, numeq=data.numeq, startpoint=rand(data.n))","category":"page"},{"location":"pop/#Methods","page":"Polynomial Optimization","title":"Methods","text":"","category":"section"},{"location":"pop/","page":"Polynomial Optimization","title":"Polynomial Optimization","text":"tssos_first\ntssos_higher!\ncs_tssos_first\ncs_tssos_higher!\nlocal_solution\nrefine_sol\nextract_solutions\nextract_solutions_robust","category":"page"},{"location":"pop/#TSSOS.tssos_first","page":"Polynomial Optimization","title":"TSSOS.tssos_first","text":"opt,sol,data = tssos_first(pop, x, d; nb=0, numeq=0, Groebnerbasis=true, basis=[], reducebasis=false, TS=\"block\", \nmerge=false, md=3, solver=\"Mosek\", QUIET=false, solve=true, MomentOne=false, Gram=false, solution=false, tol=1e-4, \ncosmo_setting=cosmo_para(), mosek_setting=mosek_para(), normality=false)\n\nCompute the first TS step of the TSSOS hierarchy for constrained polynomial optimization. If reducebasis=true, then remove monomials from the monomial basis by diagonal inconsistency. If Groebnerbasis=true, then exploit the quotient ring structure defined by the equality constraints. If merge=true, perform the PSD block merging.  If solve=false, then do not solve the SDP. If Gram=true, then output the Gram matrix. If MomentOne=true, add an extra first-order moment PSD constraint to the moment relaxation.\n\nInput arguments\n\npop: vector of the objective, inequality constraints, and equality constraints\nx: POP variables\nd: relaxation order\nnb: number of binary variables in x\nnumeq: number of equality constraints\nTS: type of term sparsity (\"block\", \"signsymmetry\", \"MD\", \"MF\", false)\nmd: tunable parameter for merging blocks\nnormality: impose the normality condtions (true, false)\nQUIET: run in the quiet mode (true, false)\ntol: relative tolerance to certify global optimality\n\nOutput arguments\n\nopt: optimum\nsol: (near) optimal solution (if solution=true)\ndata: other auxiliary data \n\n\n\n\n\nopt,sol,data = tssos_first(f, x; newton=true, reducebasis=false, TS=\"block\", merge=false, md=3, feasibility=false, solver=\"Mosek\", \nQUIET=false, solve=true, dualize=false, MomentOne=false, Gram=false, solution=false, tol=1e-4, cosmo_setting=cosmo_para(), mosek_setting=mosek_para())\n\nCompute the first TS step of the TSSOS hierarchy for unconstrained polynomial optimization. If newton=true, then compute a monomial basis by the Newton polytope method. If reducebasis=true, then remove monomials from the monomial basis by diagonal inconsistency. If TS=\"block\", use maximal chordal extensions; if TS=\"MD\", use approximately smallest chordal extensions.  If merge=true, perform the PSD block merging.  If feasibility=true, then solve the feasibility problem. If solve=false, then do not solve the SDP. If Gram=true, then output the Gram matrix. If MomentOne=true, add an extra first-order moment PSD constraint to the moment relaxation.\n\nInput arguments\n\nf: objective\nx: POP variables\nTS: type of term sparsity (\"block\", \"signsymmetry\", \"MD\", \"MF\", false)\nmd: tunable parameter for merging blocks\nQUIET: run in the quiet mode (true, false)\ntol: relative tolerance to certify global optimality\n\nOutput arguments\n\nopt: optimum\nsol: (near) optimal solution (if solution=true)\ndata: other auxiliary data \n\n\n\n\n\n","category":"function"},{"location":"pop/#TSSOS.tssos_higher!","page":"Polynomial Optimization","title":"TSSOS.tssos_higher!","text":"opt,sol,data = tssos_higher!(data; TS=\"block\", merge=false, md=3, QUIET=false, solve=true, feasibility=false, dualize=false, Gram=false,\nMomentOne=false, solution=false, cosmo_setting=cosmo_para(), mosek_setting=mosek_para(), normality=false)\n\nCompute higher TS steps of the TSSOS hierarchy.\n\n\n\n\n\n","category":"function"},{"location":"pop/#TSSOS.cs_tssos_first","page":"Polynomial Optimization","title":"TSSOS.cs_tssos_first","text":"opt,sol,data = cs_tssos_first(pop, x, d; nb=0, numeq=0, CS=\"MF\", cliques=[], basis=[], hbasis=[], TS=\"block\", merge=false, md=3, solver=\"Mosek\", \ndualize=false, QUIET=false, solve=true, solution=false, Gram=false, MomentOne=false, tol=1e-4, cosmo_setting=cosmo_para(), mosek_setting=mosek_para())\n\nCompute the first TS step of the CS-TSSOS hierarchy for constrained polynomial optimization. If merge=true, perform the PSD block merging.  If solve=false, then do not solve the SDP. If Gram=true, then output the Gram matrix. If MomentOne=true, add an extra first-order moment PSD constraint to the moment relaxation.\n\nInput arguments\n\npop: vector of the objective, inequality constraints, and equality constraints\nx: POP variables\nd: relaxation order\nnb: number of binary variables in x\nnumeq: number of equality constraints\nCS: method of chordal extension for correlative sparsity (\"MF\", \"MD\", \"NC\", false)\ncliques: the set of cliques used in correlative sparsity\nTS: type of term sparsity (\"block\", \"signsymmetry\", \"MD\", \"MF\", false)\nmd: tunable parameter for merging blocks\nQUIET: run in the quiet mode (true, false)\ntol: relative tolerance to certify global optimality\n\nOutput arguments\n\nopt: optimum\nsol: (near) optimal solution (if solution=true)\ndata: other auxiliary data \n\n\n\n\n\nopt,sol,data = cs_tssos_first(supp::Vector{Vector{Vector{UInt16}}}, coe, n, d; nb=0, numeq=0, CS=\"MF\", cliques=[], basis=[], hbasis=[], TS=\"block\", \nmerge=false, md=3, QUIET=false, solver=\"Mosek\", dualize=false, solve=true, solution=false, Gram=false, MomentOne=false, tol=1e-4, \ncosmo_setting=cosmo_para(), mosek_setting=mosek_para())\n\nCompute the first TS step of the CS-TSSOS hierarchy for constrained polynomial optimization.  Here the polynomial optimization problem is defined by supp and coe, corresponding to the supports and coeffients of pop respectively.\n\n\n\n\n\nopt,sol,data = cs_tssos_first(pop, z, n, d; nb=0, numeq=0, CS=\"MF\", cliques=[], TS=\"block\", ipart=true, reducebasis=false, \nmerge=false, md=3, solver=\"Mosek\", QUIET=false, solve=true, solution=false, dualize=false, Gram=false, balanced=false, \nMomentOne=false, normality=1, cosmo_setting=cosmo_para(), mosek_setting=mosek_para())\n\nCompute the first TS step of the CS-TSSOS hierarchy for constrained complex polynomial optimization.  If merge=true, perform the PSD block merging.  If ipart=false, then use the real moment-HSOS hierarchy. If solve=false, then do not solve the SDP. If Gram=true, then output the Gram matrix. If MomentOne=true, add an extra first-order moment PSD constraint to the moment relaxation.\n\nInput arguments\n\npop: vector of the objective, inequality constraints, and equality constraints\nz: CPOP variables and their conjugate\nn: number of CPOP variables\nd: relaxation order\nnb: number of unit-norm variables in x\nnumeq: number of equality constraints\nCS: method of chordal extension for correlative sparsity (\"MF\", \"MD\", false)\ncliques: the set of cliques used in correlative sparsity\nTS: type of term sparsity (\"block\", \"MD\", \"MF\", false)\nmd: tunable parameter for merging blocks\nnormality: normal order\nQUIET: run in the quiet mode (true, false)\n\nOutput arguments\n\nopt: optimum\nsol: (near) optimal solution (if solution=true)\ndata: other auxiliary data \n\n\n\n\n\nopt,sol,data = cs_tssos_first(supp::Vector{Vector{Vector{Vector{UInt16}}}}, coe::Vector{Vector{ComplexF64}},\nn, d; nb=0, numeq=0, CS=\"MF\", cliques=[], TS=\"block\", ipart=true, merge=false, md=3, solver=\"Mosek\", solution=false, dualize=false,\nQUIET=false, solve=true, Gram=false, balanced=false, MomentOne=false, normality=1, cosmo_setting=cosmo_para(), mosek_setting=mosek_para())\n\nCompute the first TS step of the CS-TSSOS hierarchy for constrained complex polynomial optimization.  Here the complex polynomial optimization problem is defined by supp and coe, corresponding to the supports and coeffients of pop respectively.\n\n\n\n\n\n","category":"function"},{"location":"pop/#TSSOS.cs_tssos_higher!","page":"Polynomial Optimization","title":"TSSOS.cs_tssos_higher!","text":"opt,sol,data = cs_tssos_higher!(data; TS=\"block\", merge=false, md=3, QUIET=false, solve=true, solution=false, Gram=false, dualize=false, \nMomentOne=false, cosmo_setting=cosmo_para(), mosek_setting=mosek_para())\n\nCompute higher TS steps of the CS-TSSOS hierarchy.\n\n\n\n\n\nopt,sol,data = cs_tssos_higher!(data; TS=\"block\", merge=false, md=3, QUIET=false, solve=true, ipart=true, dualize=false,\nsolution=false, Gram=false, balanced=false, MomentOne=false, cosmo_setting=cosmo_para(), mosek_setting=mosek_para(), normality=1)\n\nCompute higher TS steps of the CS-TSSOS hierarchy.\n\n\n\n\n\n","category":"function"},{"location":"pop/#TSSOS.local_solution","page":"Polynomial Optimization","title":"TSSOS.local_solution","text":"obj,sol,status = local_solution(n, m, supp::Vector{Union{Vector{Vector{UInt16}}, Array{UInt8, 2}}}, coe; nb=0, numeq=0,\nstartpoint=[], QUIET=false)\n\nCompute a local solution by a local solver.\n\nInput arguments\n\nn: number of POP variables\nm: number of POP constraints\nsupp: supports of the POP\ncoe: coefficients of the POP\nnb: number of binary variables\nnumeq: number of equality constraints\nstartpoint: provide a start point\nQUIET: run in the quiet mode or not (true, false)\n\nOutput arguments\n\nobj: local optimum\nsol: local solution\nstatus: solver termination status\n\n\n\n\n\n","category":"function"},{"location":"pop/#TSSOS.refine_sol","page":"Polynomial Optimization","title":"TSSOS.refine_sol","text":"ref_sol,flag = refine_sol(opt, sol, data, QUIET=false, tol=1e-4)\n\nRefine the obtained solution by a local solver. Return the refined solution, and flag=0 if global optimality is certified, flag=1 otherwise.\n\n\n\n\n\n","category":"function"},{"location":"pop/#TSSOS.extract_solutions","page":"Polynomial Optimization","title":"TSSOS.extract_solutions","text":"sol = extract_solutions(pop, x, d, opt, moment; numeq=0, nb=0, tol=1e-2)\n\nExtract a set of solutions for the polynomial optimization problem.\n\nInput arguments\n\npop: polynomial optimization problem\nx: set of variables\nd: relaxation order\nopt: optimum\nmoment: moment matrix\nnumeq: number of equality constraints\nnb: number of binary variables\ntol: tolerance to obtain the column echelon form\n\nOutput arguments\n\nsol: a set of solutions\n\n\n\n\n\n","category":"function"},{"location":"pop/#TSSOS.extract_solutions_robust","page":"Polynomial Optimization","title":"TSSOS.extract_solutions_robust","text":"sol = extract_solutions_robust(n, d, moment; tol=1e-2)\n\nExtract a set of solutions for the polynomial optimization problem.\n\nInput arguments\n\nn: number of variables\nd: relaxation order\nmoment: moment matrix\ntol: tolerance to obtain the column echelon form\n\nOutput arguments\n\nsol: a set of solutions\n\n\n\n\n\n","category":"function"}]
}
